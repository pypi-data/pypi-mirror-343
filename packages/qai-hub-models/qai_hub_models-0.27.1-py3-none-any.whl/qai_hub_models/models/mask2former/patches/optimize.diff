diff --git a/src/transformers/models/mask2former/modeling_mask2former.py b/src/transformers/models/mask2former/modeling_mask2former.py
index f37b5b14f..abccde6b0 100644
--- a/src/transformers/models/mask2former/modeling_mask2former.py
+++ b/src/transformers/models/mask2former/modeling_mask2former.py
@@ -802,10 +802,13 @@ def multi_scale_deformable_attention(
     value: Tensor, value_spatial_shapes: Tensor, sampling_locations: Tensor, attention_weights: Tensor
 ) -> Tensor:
     batch_size, _, num_heads, hidden_dim = value.shape
-    _, num_queries, num_heads, num_levels, num_points, _ = sampling_locations.shape
+    # _, num_queries, num_heads, num_levels, num_points, _ = sampling_locations.shape
+    num_queries, num_heads, num_points, _ = sampling_locations.shape
+    num_queries //= batch_size
     value_list = value.split([height.item() * width.item() for height, width in value_spatial_shapes], dim=1)
     sampling_grids = 2 * sampling_locations - 1
     sampling_value_list = []
+    sampling_grids = sampling_grids.split(num_points//value_spatial_shapes.shape[0],2)
     for level_id, (height, width) in enumerate(value_spatial_shapes):
         # batch_size, height*width, num_heads, hidden_dim
         # -> batch_size, height*width, num_heads*hidden_dim
@@ -817,7 +820,8 @@ def multi_scale_deformable_attention(
         # batch_size, num_queries, num_heads, num_points, 2
         # -> batch_size, num_heads, num_queries, num_points, 2
         # -> batch_size*num_heads, num_queries, num_points, 2
-        sampling_grid_l_ = sampling_grids[:, :, :, level_id].transpose(1, 2).flatten(0, 1)
+        # sampling_grid_l_ = sampling_grids[:, :, :, level_id].transpose(1, 2).flatten(0, 1)
+        sampling_grid_l_ = sampling_grids[level_id].transpose(0,1)
         # batch_size*num_heads, hidden_dim, num_queries, num_points
         sampling_value_l_ = nn.functional.grid_sample(
             value_l_, sampling_grid_l_, mode="bilinear", padding_mode="zeros", align_corners=False
@@ -827,10 +831,12 @@ def multi_scale_deformable_attention(
     # -> (batch_size, num_heads, num_queries, num_levels, num_points)
     # -> (batch_size, num_heads, 1, num_queries, num_levels*num_points)
     attention_weights = attention_weights.transpose(1, 2).reshape(
-        batch_size * num_heads, 1, num_queries, num_levels * num_points
+        # batch_size * num_heads, 1, num_queries, num_levels * num_points
+        batch_size * num_heads, 1, num_queries, num_points
     )
     output = (
-        (torch.stack(sampling_value_list, dim=-2).flatten(-2) * attention_weights)
+        # (torch.stack(sampling_value_list, dim=-2).flatten(-2) * attention_weights)
+        (torch.concat(sampling_value_list, dim=-1) * attention_weights)
         .sum(-1)
         .view(batch_size, num_heads * hidden_dim, num_queries)
     )
@@ -942,20 +948,29 @@ class Mask2FormerPixelDecoderEncoderMultiscaleDeformableAttention(nn.Module):
             value = value.masked_fill(attention_mask[..., None], float(0))
         value = value.view(batch_size, sequence_length, self.n_heads, self.d_model // self.n_heads)
         sampling_offsets = self.sampling_offsets(hidden_states).view(
-            batch_size, num_queries, self.n_heads, self.n_levels, self.n_points, 2
+            # batch_size, num_queries, self.n_heads, self.n_levels, self.n_points, 2
+            batch_size* num_queries, self.n_heads, self.n_levels* self.n_points, 2
         )
         attention_weights = self.attention_weights(hidden_states).view(
             batch_size, num_queries, self.n_heads, self.n_levels * self.n_points
         )
-        attention_weights = nn.functional.softmax(attention_weights, -1).view(
-            batch_size, num_queries, self.n_heads, self.n_levels, self.n_points
-        )
+        attention_weights = nn.functional.softmax(attention_weights, -1)#.view(
+        #     batch_size, num_queries, self.n_heads, self.n_levels, self.n_points
+        # )
         # batch_size, num_queries, n_heads, n_levels, n_points, 2
         if reference_points.shape[-1] == 2:
             offset_normalizer = torch.stack([spatial_shapes[..., 1], spatial_shapes[..., 0]], -1)
+
+            # sampling_locations = (
+            #     reference_points[:, :, None, :, None, :]
+            #     + sampling_offsets / offset_normalizer[None, None, None, :, None, :]
+            # )
+            reference_points = reference_points.reshape(-1,self.n_levels,1, 2).repeat(1,1,self.n_points,1).reshape(-1,1,self.n_levels*self.n_points,2)
             sampling_locations = (
-                reference_points[:, :, None, :, None, :]
-                + sampling_offsets / offset_normalizer[None, None, None, :, None, :]
+                # reference_points[:, :, None, :, None, :]
+                # + sampling_offsets / offset_normalizer[None, None, None, :, None, :]
+                reference_points
+                + sampling_offsets / offset_normalizer.unsqueeze(-2).repeat(1,self.n_points,1).reshape(1,1,-1,2)
             )
         elif reference_points.shape[-1] == 4:
             sampling_locations = (
@@ -1326,14 +1341,19 @@ class Mask2FormerPixelDecoder(nn.Module):
         last_hidden_state = encoder_outputs.last_hidden_state
         batch_size = last_hidden_state.shape[0]
 
+        # We compute level_start_index_list separately from the tensor version level_start_index
+        # to avoid iterating over a tensor which breaks torch.compile/export.
+        level_start_index_list = [0]
+        for height, width in spatial_shapes[:-1]:
+            level_start_index_list.append(level_start_index_list[-1] + height * width)
         split_sizes = [None] * self.num_feature_levels
         for i in range(self.num_feature_levels):
             if i < self.num_feature_levels - 1:
-                split_sizes[i] = level_start_index[i + 1] - level_start_index[i]
+                split_sizes[i] = level_start_index_list[i + 1] - level_start_index_list[i]
             else:
-                split_sizes[i] = last_hidden_state.shape[1] - level_start_index[i]
+                split_sizes[i] = last_hidden_state.shape[1] - level_start_index_list[i]
 
-        encoder_output = torch.split(last_hidden_state, [size.item() for size in split_sizes], dim=1)
+        encoder_output = torch.split(last_hidden_state, split_sizes, dim=1)
 
         # Compute final features
         outputs = [
@@ -1871,7 +1891,10 @@ class Mask2FormerMaskedAttentionDecoder(nn.Module):
             else:
                 level_index = idx % self.num_feature_levels
 
-                attention_mask[torch.where(attention_mask.sum(-1) == attention_mask.shape[-1])] = False
+                # attention_mask[torch.where(attention_mask.sum(-1) == attention_mask.shape[-1])] = False
+                where = (attention_mask.sum(-1) != attention_mask.shape[-1]).to(attention_mask.dtype)
+                # Multiply the attention mask instead of indexing to avoid issue in torch.export.
+                attention_mask = attention_mask * where.unsqueeze(-1)
 
                 layer_outputs = decoder_layer(
                     hidden_states,
@@ -2005,16 +2028,18 @@ class Mask2FormerMaskPredictor(nn.Module):
             or (hasattr(torch, "_dynamo") and torch._dynamo.is_compiling())
         )
         # Sum up over the channels
-        if is_tracing and not is_torch_greater_or_equal_than_2_1:
-            # Equivalent to einsum('bqc, bchw -> bqhw') but jit friendly
-            batch_size, num_queries, num_channels = mask_embeddings.shape
-            _, _, height, width = pixel_embeddings.shape
-            outputs_mask = torch.zeros((batch_size, num_queries, height, width), device=mask_embeddings.device)
-            for c in range(num_channels):
-                outputs_mask += mask_embeddings[..., c][..., None, None] * pixel_embeddings[:, None, c]
-
-        else:
-            outputs_mask = torch.einsum("bqc, bchw -> bqhw", mask_embeddings, pixel_embeddings)
+        # if is_tracing and not is_torch_greater_or_equal_than_2_1:
+        # Equivalent to einsum('bqc, bchw -> bqhw') but jit friendly
+        batch_size, num_queries, num_channels = mask_embeddings.shape
+        _, _, height, width = pixel_embeddings.shape
+        # outputs_mask = torch.zeros((batch_size, num_queries, height, width), device=mask_embeddings.device)
+        # for c in range(num_channels):
+        #     outputs_mask += mask_embeddings[..., c][..., None, None] * pixel_embeddings[:, None, c]
+        pixel_embeddings = pixel_embeddings.reshape(batch_size,num_channels,-1)
+        outputs_mask = torch.matmul(mask_embeddings, pixel_embeddings).reshape(batch_size,num_queries,height, width)
+
+        # else:
+        #     outputs_mask = torch.einsum("bqc, bchw -> bqhw", mask_embeddings, pixel_embeddings)
 
         attention_mask = nn.functional.interpolate(
             outputs_mask, size=attention_mask_target_size, mode="bilinear", align_corners=False
diff --git a/src/transformers/models/swin/modeling_swin.py b/src/transformers/models/swin/modeling_swin.py
index cb0eff88a..b666a8e7f 100644
--- a/src/transformers/models/swin/modeling_swin.py
+++ b/src/transformers/models/swin/modeling_swin.py
@@ -215,9 +215,11 @@ def window_partition(input_feature, window_size):
     """
     batch_size, height, width, num_channels = input_feature.shape
     input_feature = input_feature.view(
-        batch_size, height // window_size, window_size, width // window_size, window_size, num_channels
+        # batch_size, height // window_size, window_size, width // window_size, window_size, num_channels
+        batch_size*height // window_size, window_size, width // window_size, window_size*num_channels
     )
-    windows = input_feature.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, num_channels)
+    # windows = input_feature.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, num_channels)
+    windows = input_feature.permute(0, 2, 1, 3).contiguous().view(-1, window_size, window_size, num_channels)
     return windows
 
 
@@ -226,8 +228,10 @@ def window_reverse(windows, window_size, height, width):
     Merges windows to produce higher resolution features.
     """
     num_channels = windows.shape[-1]
-    windows = windows.view(-1, height // window_size, width // window_size, window_size, window_size, num_channels)
-    windows = windows.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, height, width, num_channels)
+    # windows = windows.view(-1, height // window_size, width // window_size, window_size, window_size, num_channels)
+    # windows = windows.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, height, width, num_channels)
+    windows = windows.view(-1, width // window_size, window_size, window_size*num_channels)
+    windows = windows.permute(0, 2, 1, 3).contiguous().view(-1, height, width, num_channels)
     return windows
 
 
@@ -484,10 +488,11 @@ class SwinSelfAttention(nn.Module):
             # Apply the attention mask is (precomputed for all layers in SwinModel forward() function)
             mask_shape = attention_mask.shape[0]
             attention_scores = attention_scores.view(
-                batch_size // mask_shape, mask_shape, self.num_attention_heads, dim, dim
+                # batch_size // mask_shape, mask_shape, self.num_attention_heads, dim, dim
+                -1, self.num_attention_heads, dim, dim
             )
-            attention_scores = attention_scores + attention_mask.unsqueeze(1).unsqueeze(0)
-            attention_scores = attention_scores.view(-1, self.num_attention_heads, dim, dim)
+            attention_scores = attention_scores + attention_mask.unsqueeze(1)#.unsqueeze(0)
+            # attention_scores = attention_scores.view(-1, self.num_attention_heads, dim, dim)
 
         # Normalize the attention scores to probabilities.
         attention_probs = nn.functional.softmax(attention_scores, dim=-1)
