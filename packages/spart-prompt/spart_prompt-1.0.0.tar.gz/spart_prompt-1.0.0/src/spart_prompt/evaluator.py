from spart_prompt.llm_connector import LLMConnector
from concurrent.futures import ThreadPoolExecutor
from sentence_transformers import SentenceTransformer, util
from rouge_score import rouge_scorer
import os

# Disable parallelism warnings from tokenizers
os.environ["TOKENIZERS_PARALLELISM"] = "false"


class PromptEvaluator:
    """
    A class to evaluate the effectiveness of system prompts based on similarity metrics.

    This class leverages semantic and syntactic similarity measures to assess the quality
    of generated responses from a language model (LLM) in comparison to desired outputs.

    Attributes:
        llm (callable): A function or API connector for generating responses from the LLM.
        model (SentenceTransformer): Pretrained transformer model for semantic similarity.
    """

    def __init__(self, llm: LLMConnector) -> None:
        """
        Initializes the PromptEvaluator with a given LLM and necessary models.

        Args:
            llm (callable): The function or API used to generate responses from the LLM.
        """
        self.llm = llm
        self.model = SentenceTransformer(
            'all-MiniLM-L6-v2')  # Semantic similarity model

    def _generate_prompt_output(
            self,
            input_data: str,
            generated_prompt: str) -> str:
        """
        Generates output for the given prompt using the LLM.

        Args:
            input_data (str): The input data for which a response is generated.
            generated_prompt (str): The prompt that guides the LLM's response.

        Returns:
            str: The output generated by the LLM.
        """
        recommended_prompt = f'''
            {generated_prompt}
            {{input_data}} = {input_data}
        '''
        return self.llm(recommended_prompt)

    def _compute_semantic_similarity(
            self,
            desired_output: str,
            prompt_output: str) -> float:
        """
        Computes the cosine similarity between the desired and generated outputs.

        Args:
            desired_output (str): The expected output.
            prompt_output (str): The output generated by the LLM.

        Returns:
            float: The cosine similarity score between 0 and 1 (higher is better).
        """
        desired_embedding = self.model.encode(
            desired_output, convert_to_tensor=True)
        prompt_embedding = self.model.encode(
            prompt_output, convert_to_tensor=True)
        return util.pytorch_cos_sim(desired_embedding, prompt_embedding).item()

    def _compute_syntactic_similarity(
            self,
            desired_output: str,
            prompt_output: str) -> float:
        """
        Computes the syntactic similarity between the desired and generated outputs using ROUGE-L.

        Args:
            desired_output (str): The expected output.
            prompt_output (str): The output generated by the LLM.

        Returns:
            float: The ROUGE-L F1 score between 0 and 1 (higher is better).
        """
        scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
        return scorer.score(desired_output, prompt_output)['rougeL'].fmeasure

    def _evaluate_single_pair(
        self,
        input_data: str,
        desired_output: str,
        generated_prompt: str,
        use_semantic_similarity: bool,
        use_syntactic_similarity: bool
    ) -> tuple[float, float, str]:
        """
        Evaluates a single input-output pair for similarity.

        Args:
            input_data (str): The input example.
            desired_output (str): The expected output for the input.
            generated_prompt (str): The optimized prompt guiding the LLM response.
            use_semantic_similarity (bool): Whether to compute semantic similarity.
            use_syntactic_similarity (bool): Whether to compute syntactic similarity.

        Returns:
            tuple: (cosine similarity, ROUGE-L score, generated output).
        """
        prompt_output = self._generate_prompt_output(
            input_data, generated_prompt)

        cosine_similarity = self._compute_semantic_similarity(
            desired_output, prompt_output) if use_semantic_similarity else -1
        rouge_score = self._compute_syntactic_similarity(
            desired_output, prompt_output) if use_syntactic_similarity else -1

        return cosine_similarity, rouge_score, prompt_output

    def evaluate_similarity(
        self,
        input_column: list[str],
        output_column: list[str],
        generated_prompt: str,
        use_semantic_similarity: bool = True,
        use_syntactic_similarity: bool = True
    ) -> tuple[float, float, list[str]]:
        """
        Evaluates the effectiveness of a prompt using multiple input-output pairs.

        Args:
            input_column (list of str): List of input examples.
            output_column (list of str): List of expected outputs corresponding to inputs.
            generated_prompt (str): The optimized prompt used for response generation.
            use_semantic_similarity (bool, optional): Whether to compute semantic similarity. Defaults to True.
            use_syntactic_similarity (bool, optional): Whether to compute syntactic similarity. Defaults to True.

        Returns:
            tuple: (average semantic similarity, average syntactic similarity, list of generated outputs).
        """
        cosine_similarities, rouge_scores, outputs = [], [], []

        # Evaluate input-output pairs in parallel for efficiency
        with ThreadPoolExecutor() as executor:
            futures = [
                executor.submit(
                    self._evaluate_single_pair,
                    input_data,
                    desired_output,
                    generated_prompt,
                    use_semantic_similarity,
                    use_syntactic_similarity) for input_data,
                desired_output in zip(
                    input_column,
                    output_column)]
            results = [future.result() for future in futures]

        # Extract results from parallel execution
        cosine_similarities = [result[0] for result in results]
        rouge_scores = [result[1] for result in results]
        outputs = [result[2] for result in results]

        # Compute average similarity scores
        avg_cosine_similarity = sum(
            cosine_similarities) / len(cosine_similarities) if use_semantic_similarity else -1
        avg_rouge_score = sum(
            rouge_scores) / len(rouge_scores) if use_syntactic_similarity else -1

        return avg_cosine_similarity, avg_rouge_score, outputs
