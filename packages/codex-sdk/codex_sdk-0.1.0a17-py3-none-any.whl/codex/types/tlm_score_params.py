# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

from __future__ import annotations

from typing import List, Iterable, Optional
from typing_extensions import Literal, Required, TypedDict

__all__ = ["TlmScoreParams", "Options"]


class TlmScoreParams(TypedDict, total=False):
    prompt: Required[str]

    response: Required[str]

    constrain_outputs: Optional[List[str]]

    options: Optional[Options]
    """
    Typed dict of advanced configuration options for the Trustworthy Language Model.
    Many of these configurations are determined by the quality preset selected
    (learn about quality presets in the TLM [initialization method](./#class-tlm)).
    Specifying TLMOptions values directly overrides any default values set from the
    quality preset.

    For all options described below, higher settings will lead to longer runtimes
    and may consume more tokens internally. You may not be able to run long prompts
    (or prompts with long responses) in your account, unless your token/rate limits
    are increased. If you hit token limit issues, try lower/less expensive
    TLMOptions to be able to run longer prompts/responses, or contact Cleanlab to
    increase your limits.

    The default values corresponding to each quality preset are:

    - **best:** `num_candidate_responses` = 6, `num_consistency_samples` = 8,
      `use_self_reflection` = True. This preset improves LLM responses.
    - **high:** `num_candidate_responses` = 4, `num_consistency_samples` = 8,
      `use_self_reflection` = True. This preset improves LLM responses.
    - **medium:** `num_candidate_responses` = 1, `num_consistency_samples` = 8,
      `use_self_reflection` = True.
    - **low:** `num_candidate_responses` = 1, `num_consistency_samples` = 4,
      `use_self_reflection` = True.
    - **base:** `num_candidate_responses` = 1, `num_consistency_samples` = 0,
      `use_self_reflection` = False. When using `get_trustworthiness_score()` on
      "base" preset, a cheaper self-reflection will be used to compute the
      trustworthiness score.

    By default, the TLM uses the "medium" quality preset. The default base LLM
    `model` used is "gpt-4o-mini", and `max_tokens` is 512 for all quality presets.
    You can set custom values for these arguments regardless of the quality preset
    specified.

    Args: model ({"gpt-4o-mini", "gpt-4o", "o3-mini", "o1", "o1-mini", "o1-preview",
    "gpt-3.5-turbo-16k", "gpt-4", "gpt-4.5-preview", "claude-3.7-sonnet",
    "claude-3.5-sonnet-v2", "claude-3.5-sonnet", "claude-3.5-haiku",
    "claude-3-haiku", "nova-micro", "nova-lite", "nova-pro"}, default =
    "gpt-4o-mini"): Underlying base LLM to use (better models yield better results,
    faster models yield faster/cheaper results). - Models still in beta: "o1",
    "o3-mini", "o1-mini", "gpt-4.5-preview", "claude-3.7-sonnet",
    "claude-3.5-sonnet-v2", "claude-3.5-haiku", "nova-micro", "nova-lite",
    "nova-pro". - Recommended models for accuracy: "gpt-4o", "o3-mini", "o1",
    "claude-3.7-sonnet". - Recommended models for low latency/costs: "nova-micro",
    "gpt-4o-mini".

        max_tokens (int, default = 512): the maximum number of tokens that can be generated in the TLM response (and in internal trustworthiness scoring).
        Higher values here may produce better (more reliable) TLM responses and trustworthiness scores, but at higher costs/runtimes.
        If you experience token/rate limit errors while using TLM, try lowering this number.
        For OpenAI models, this parameter must be between 64 and 4096. For Claude models, this parameter must be between 64 and 512.

        num_candidate_responses (int, default = 1): how many alternative candidate responses are internally generated by TLM.
        TLM scores the trustworthiness of each candidate response, and then returns the most trustworthy one.
        Higher values here can produce better (more accurate) responses from the TLM, but at higher costs/runtimes (and internally consumes more tokens).
        This parameter must be between 1 and 20.
        When it is 1, TLM simply returns a standard LLM response and does not attempt to auto-improve it.

        num_consistency_samples (int, default = 8): the amount of internal sampling to evaluate LLM response consistency.
        Must be between 0 and 20. Higher values produce more reliable TLM trustworthiness scores, but at higher costs/runtimes.
        This consistency helps quantify the epistemic uncertainty associated with
        strange prompts or prompts that are too vague/open-ended to receive a clearly defined 'good' response.
        TLM internally measures consistency via the degree of contradiction between sampled responses that the model considers equally plausible.

        use_self_reflection (bool, default = `True`): whether the LLM is asked to self-reflect upon the response it
        generated and self-evaluate this response.
        Setting this False disables self-reflection and may worsen trustworthiness scores, but will reduce costs/runtimes.
        Self-reflection helps quantify aleatoric uncertainty associated with challenging prompts
        and catches answers that are obviously incorrect/bad.

        similarity_measure ({"semantic", "string", "embedding", "embedding_large"}, default = "semantic"): how the trustworthiness scoring algorithm measures
        similarity between sampled responses considered by the model in the consistency assessment.
        Supported similarity measures include "semantic" (based on natural language inference), "string" (based on character/word overlap),
        "embedding" (based on embedding similarity), and "embedding_large" (based on embedding similarity with a larger embedding model).
        Set this to "string" to improve latency/costs.

        reasoning_effort ({"none", "low", "medium", "high"}, default = "high"): how much the LLM can reason (number of thinking tokens)
        when considering alternative possible responses and double-checking responses.
        Higher efforts here may produce better TLM trustworthiness scores and LLM responses. Reduce this value to improve latency/costs.

        log (list[str], default = []): optionally specify additional logs or metadata that TLM should return.
        For instance, include "explanation" here to get explanations of why a response is scored with low trustworthiness.

        custom_eval_criteria (list[dict[str, Any]], default = []): optionally specify custom evalution criteria.
        The expected input format is a list of dictionaries, where each dictionary has the following keys:
        - name: Name of the evaluation criteria.
        - criteria: Instructions specifying the evaluation criteria.
    """

    quality_preset: Literal["best", "high", "medium", "low", "base"]

    task: Optional[str]


class Options(TypedDict, total=False):
    custom_eval_criteria: Iterable[object]

    log: List[str]

    max_tokens: int

    model: str

    num_candidate_responses: int

    num_consistency_samples: int

    reasoning_effort: str

    similarity_measure: str

    use_self_reflection: bool
