Metadata-Version: 2.4
Name: data-flow-generator
Version: 0.1.2
Summary: Interactive data flow visualization tool for database dumps
Author-email: Jon-Mikkel Korsvik <jkorsvik@outlook.com>
License: MIT
Project-URL: Homepage, https://github.com/jkorsvik/dataflow-generator
Project-URL: Documentation, https://github.com/jkorsvik/dataflow-generator/blob/main/README.md
Keywords: dataflow,visualization,vql,denodo,diagram
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.12
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: colorama==0.4.6
Requires-Dist: kaleido==0.2.1
Requires-Dist: narwhals==1.33.0
Requires-Dist: networkx==3.3
Requires-Dist: numpy>=2.2.4
Requires-Dist: packaging>=24.2
Requires-Dist: plotly==6.0.1
Requires-Dist: python-dotenv==1.0.1
Requires-Dist: pyvis>=0.3.2
Requires-Dist: rapidfuzz==3.6.1
Requires-Dist: readchar==4.1.0
Requires-Dist: questionary>=2.0.1
Requires-Dist: mypy>=1.15.0
Requires-Dist: pytest-cov==6.1.0
Requires-Dist: coverage-badge>=1.1.2
Requires-Dist: anybadge>=1.16.0
Requires-Dist: pandas-stubs>=2.2.3.250308
Requires-Dist: types-pygments>=2.19.0.20250305
Requires-Dist: types-colorama>=0.4.15.20240311
Requires-Dist: types-networkx>=3.4.2.20250319
Provides-Extra: dev
Requires-Dist: mypy>=1.15.0; extra == "dev"
Requires-Dist: pytest>=8.3.5; extra == "dev"
Requires-Dist: pytest-cov>=4.0; extra == "dev"
Dynamic: license-file

# Data Flow Visualization Tool

[![CI](https://github.com/jkorsvik/dataflow/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/jkorsvik/dataflow/actions/workflows/ci.yml)
[![Tests](https://github.com/jkorsvik/dataflow/actions/workflows/test.yml/badge.svg?branch=main)](https://github.com/jkorsvik/dataflow/actions/workflows/test.yml)
[![Coverage](https://github.com/jkorsvik/dataflow/raw/main/coverage.svg)](https://github.com/jkorsvik/dataflow/actions/workflows/ci.yml)

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python](https://img.shields.io/badge/Python-3.12%2B-blue.svg)](https://www.python.org/)
[![SQL](https://img.shields.io/badge/SQL-Supported-green.svg)](https://en.wikipedia.org/wiki/SQL)
[![JavaScript](https://img.shields.io/badge/JavaScript-ES6%2B-yellow.svg)](https://developer.mozilla.org/en-US/docs/Web/JavaScript)


This tool generates visual representations of data flows based on Denodo metadata exported in a `.vql` file. It can produce both complete data flow diagrams and focused data flow diagrams for specified views and tables.

## Changelog

### 0.1.1 (2025-04-24)
- Bumped project version to 0.1.1
- Added keywords and long_description_content_type in pyproject.toml for PyPI
- Switched CI publish step to use Twine upload

## Installation

### Dependencies

- Python 3.10 or higher
- UV package manager

### Setup (Works also for openvscodeserver)

If you prefer not to use UV, you can still use the traditional setup:
```sh
source setup.sh
```

To reset the environment:
```sh
rm -rf .venv && source setup.sh
```

## Installation

- Install from PyPI:
```sh
uv pip install data-flow-generator
```

- Install for current user:
```sh
uv pip install --user data-flow-generator
```

## Usage

1. **Install the Tool:**

   You can install the tool globally using UV:

   ```sh
   # Install globally with UV
   uv pip install . 
   ```

   Or install for your user only:

   ```sh
   # Install for current user
   uv pip install --user .
   ```

2. **Run the Tool:**

   After installation, you can run the tool from anywhere using:

   ```sh
   dataflow
   ```

   The tool provides multiple ways to select your SQL file:
   - Drop/Upload File: Simply drag and drop any SQL file into the terminal
   - Browse SQL Files: Shows a filtered list of SQL files in the directory
   - Specify file path: Enter or paste a file path directly
   - Search in directory: Search for SQL files by name

   Supported SQL file extensions:
   - `.sql` - Standard SQL files
   - `.vql` - Denodo VQL files
   - `.ddl` - Data Definition Language
   - `.dml` - Data Manipulation Language
   - `.hql` - Hive Query Language
   - `.pls`, `.plsql` - PL/SQL files
   - `.proc` - Stored Procedures
   - `.psql` - PostgreSQL files
   - `.tsql` - T-SQL files
   - `.view` - View definitions

   File Validation:
   - Automatically checks for valid SQL extensions
   - Validates file content for SQL keywords
   - Provides option to proceed with non-SQL files after confirmation

   The tool generates diagrams in the "generated-image" folder:
   - Complete flow diagram: Shows all dependencies
   - Focused flow diagram: Shows selected nodes and their relationships

   Outputs include:
   - PNG files for static visualization
   - Interactive HTML files for dynamic exploration

2. **Database Selection:**

   The tool now analyzes database names in your metadata and prompts you to select a main database. 
   Databases are presented in descending order by frequency, with the most common database listed first.

   - The main database will be displayed without a prefix in the visualization
   - Data Market objects are prefixed with "datamarket." (e.g., "datamarket.bv_datalake_d1300_currency_full")
   - All other database objects are prefixed with "other."

   This database differentiation is represented in the visualization legend with different colors.

   ## Development

   For contributors, it's recommended to install the tool in editable mode. This allows you to modify the code and see changes immediately without reinstalling.

   ### Editable Installation

   Activate your virtual environment and run:

   ```sh
   uv pip install -e ".[dev]"
   ```

   This installs the package in editable mode, making it easier to test changes during development.

   ### Tips for Local Development

   - Ensure your virtual environment is activated before running any commands.
   - Use `pytest` to continuously run tests as you make changes.
   - Consider setting up linting and format checking to maintain code quality.


## Testing

The project uses pytest for testing with separate unit and integration tests. The testing setup ensures high code quality and proper functionality:

### Running Tests

With your virtual environment activated:

```sh
# Run all tests (unit + integration)
pytest

# Run only unit tests
pytest tests/unit/

# Run only integration tests
pytest tests/integration/

# Run with coverage report
pytest --cov=src --cov-report=term

# Run specific test file
pytest tests/unit/test_graph_generation.py
```

### Test Structure

1. Unit Tests (`tests/unit/`):
   - VQL parsing and graph generation
   - Node type inference and database detection
   - CLI core functionality
   - Visualization output validation
   - Error handling and edge cases

2. Integration Tests (`tests/integration/`):
   - End-to-end workflow testing
   - File generation and validation
   - Complex graph scenarios

### Coverage Requirements

A minimum of 80% code coverage is maintained across all modules. The CI pipeline enforces this requirement and generates coverage badges automatically.

## Script Overview

### Parsing `.vql` File

The script reads and parses the `.vql` file to extract metadata about views and tables. It uses regular expressions to find:
- **Views**: Matched by the pattern `CREATE OR REPLACE (INTERFACE) VIEW`.
- **Tables**: Matched by the pattern `CREATE OR REPLACE TABLE`.

Dependencies between views and tables are identified using the `FROM` and `JOIN` keywords within the view's and table's definitions.

### Database Identification

The tool now identifies database prefixes in object names (e.g., "data_market.table_name") and processes them as follows:
- Objects from the main database (selected by the user) are displayed without a prefix
- Objects from "data_market" are displayed with the prefix "datamarket."
- Objects from all other databases are displayed with the prefix "other."

This helps you quickly identify objects that come from different databases in your visualization.

### Handling Files Without Database Prefixes

If the tool does not detect any database prefixes in your VQL file, it will:
1. Use the filename as the default database name (with special characters removed)
2. Allow you to select this as the main database
3. Process all objects as if they belong to this main database

### Functions

1. **find_script_dependencies(vql_script)**:
   - Finds all the table names a `vql_script` is dependent on using the `FROM` and `JOIN` keywords.
   - Returns a list of names (dependencies in the given `vql_script`).

2. **parse_vql(file_path)**:
   - Parses the `.vql` file to extract views, tables, and their dependencies.
   - Returns a list of edges (dependencies), a dictionary of node types, and database counts.

3. **standardize_database_names(edges, node_types, main_db)**:
   - Standardizes database names based on the selected main database.
   - Formats node names to include database prefixes where appropriate.

4. **create_pyvis_figure(graph, node_types, focus_nodes=[], shake_towards_root=False)**:
   - Creates interactive Pyvis figures for the data flow diagrams.
   - Returns an interactive figure.

5. **generate_plotly_png(edges, node_types, ...)**:
   - Creates Plotly figures for the data flow diagrams with database information.
   - Returns a static figure.

6. **draw_complete_data_flow(edges, node_types, save_path=None, file_name=None)**:
   - Generates and displays a complete data flow diagram.
   - Adjusts the figure size based on the number of nodes.
   - Saves the figure as `complete_data_flow_plotly_metadata_file_name.png` and `complete_data_flow_pyvis_metadata_file_name.html`.

7. **draw_focused_data_flow(edges, node_types, focus_nodes, save_path=None, file_name=None, see_ancestors=True, see_descendants=True)**:
   - Generates and displays a focused data flow diagram for the specified nodes.
   - Includes the specified nodes, their ancestors, and descendants in the subgraph if enabled.
   - Adjusts the figure size based on the number of nodes.
   - Saves the figure as `focused_data_flow_plotly_metadata_file_name.png` and `focused_data_flow_pyvis_metadata_file_name.html`.

### Main Script Execution

- Reads the `.vql` file in the metadata folder selected using the CLI-tool
- Parses the file to extract metadata (views, tables, and their dependencies).
- Prompts the user to select the main database from a list ordered by frequency.
- Standardizes node names based on database information.
- Generates and saves the complete data flow diagram if selected in the CLI-tool.
- Generates and saves the focused data flow diagram if selected in the CLI-tool.

## Project Structure

```
/data-flow-generator
    |-- pyproject.toml          # Project configuration and dependencies
    
    |-- requirements.txt        # Legacy requirements file
    |-- uv.lock                # UV lock file
    |-- src/
    |   |-- __init__.py
    |   |-- dataflow.py         # Command line interface
    |   |-- generate_data_flow.py
    |   |-- plotly_vis.py
    |   |-- pyvis_mod.py
    |-- tests/
    |   |-- generate_data_flow_test.py
    |   |-- test_database_functions.py
    |-- metadata/              # VQL file directory
    |   |-- denodo_metadata1.vql
    |   |-- denodo_metadata2.vql
    |-- generated-image/       # Output directory
        |-- complete_data_flow_*.png
        |-- focused_data_flow_*.png
        |-- complete_data_flow_*.html
        |-- focused_data_flow_*.html
```

## Troubleshooting

- **File Not Found Error**:
  - Ensure the `.vql` file is in the metadata folder within the same directory as the script.

- **Overlapping Titles in Diagram**:
  - Increase the `SCALING_CONSTANT` value in `generate_data_flow.py` file to widen the figure.

- **Legend overlaps nodes in diagram**:
  - As the generation of the diagram is not deterministic, rerun the generation untill a desired output is achieved


If you encounter any issues or need further assistance, feel free to ask us at Insight Factory, Emerging Business Technology!
