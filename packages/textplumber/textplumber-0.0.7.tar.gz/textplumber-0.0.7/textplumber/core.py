"""Helper functions for textplumber."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/95_core.ipynb.

# %% ../nbs/95_core.ipynb 3
from __future__ import annotations
import os

# %% auto 0
__all__ = ['pass_tokens', 'get_stop_words', 'get_example_data']

# %% ../nbs/95_core.ipynb 4
def pass_tokens(tokens:list 
				) -> list: 
	""" Pass through function so pre-tokenized input can be passed to CountVectorizer or TfidfVectorizer. """
	return tokens

# %% ../nbs/95_core.ipynb 5
def get_stop_words(save_to:str|None = 'lexicons_stop_words.txt' # where to save the file, None will not save
					):
	""" Get stop words from NLTK (with option to cache to disk). """
	if save_to is not None:
		if os.path.exists(save_to):
			with open(save_to, 'r', encoding='utf-8') as f:
				stop_words = f.read().splitlines()
				return stop_words
		else:
			save_path = os.path.dirname(save_to)
			if save_path != '' and not os.path.exists(save_path):
				os.makedirs(save_path)

	import nltk
	from nltk.corpus import stopwords
	nltk.download('stopwords')
	stop_words = stopwords.words('english')

	if save_to is not None:
		with open(save_to, 'w', encoding='utf-8') as f:
			for word in stopwords.words('english'):
				f.write(word + '\n')

	return stop_words


# %% ../nbs/95_core.ipynb 7
def get_example_data(
		train_split_name:str = 'train', # this can be defined, but probably unnecessary to change
		test_split_name:str = 'validation', # could be 'test'
		label_column:str = 'category', # 'category' or 'style'
		target_labels:list = ['blog', 'author', 'speech'] # see the dataset card for information on labels https://huggingface.co/datasets/hallisky/AuthorMix
		):
	""" Get data for examples using Huggingface dataset hallisky/AuthorMix. Majority classes are automatically undersampled. """

	import numpy as np
	from datasets import load_dataset, ClassLabel
	from imblearn.under_sampling import RandomUnderSampler

	text_column = 'text'

	dataset = load_dataset('hallisky/AuthorMix')

	class_feature = ClassLabel(names=dataset['train'].unique(label_column))
	for split in dataset.keys():
		dataset[split] = dataset[split].cast_column(label_column, class_feature)

	label_names = dataset['train'].features[label_column].names

	target_classes = [label_names.index(name) for name in target_labels]
	target_names = [label_names[i] for i in target_classes]

	X_train = np.array(dataset[train_split_name][text_column])
	y_train = np.array(dataset[train_split_name][label_column])
	X_test = np.array(dataset[test_split_name][text_column])
	y_test = np.array(dataset[test_split_name][label_column])

	mask = np.isin(y_train, target_classes)
	mask_test = np.isin(y_test, target_classes)

	X_train = X_train[mask]
	y_train = y_train[mask]
	X_test = X_test[mask_test]
	y_test = y_test[mask_test]

	# undersampling all but the minority class to balance the training data
	X_train = X_train.reshape(-1, 1)
	X_train, y_train = RandomUnderSampler(random_state=0).fit_resample(X_train, y_train)
	X_train = X_train.reshape(-1)

	return X_train, y_train, X_test, y_test, target_classes, target_names

