"""Store text features to avoid recomputing them."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/90_store.ipynb.

# %% ../nbs/90_store.ipynb 3
from __future__ import annotations
from fastcore.basics import patch
import pickle
import os
import hashlib
import sqlite3
import string
import pandas as pd

# %% auto 0
__all__ = ['TextFeatureStore']

# %% ../nbs/90_store.ipynb 4
class TextFeatureStore:
	""" A class to store features extracted for a text classification pipeline and cache them to disk to avoid recomputing them. """
	def __init__(self, 
			  	 path:str  # where to save SQLite db to persist the feature store between runs
				 ):
		self.texts = {}
		self.path = path

		if self.path is not None:
			if os.path.dirname(self.path) != '':
				if not os.path.exists(os.path.dirname(self.path)):
					os.makedirs(os.path.dirname(self.path))
		else:
			raise ValueError("The feature store requires a path")

		with sqlite3.connect(self.path) as conn:
			conn.execute('''CREATE TABLE IF NOT EXISTS texts
				(hash TEXT PRIMARY KEY, tokens BLOB NOT NULL, pos BLOB NOT NULL, textstats BLOB NOT NULL) WITHOUT ROWID;''')
			conn.execute('''CREATE TABLE IF NOT EXISTS embeddings
				(hash TEXT PRIMARY KEY, embeddings BLOB NOT NULL) WITHOUT ROWID;''')		
			conn.execute('''CREATE TABLE IF NOT EXISTS lexicons
				(hash TEXT PRIMARY KEY, lexicons BLOB NOT NULL) WITHOUT ROWID;''')	
			conn.commit()

# %% ../nbs/90_store.ipynb 9
@patch
def dump(self:TextFeatureStore, 
		 structure_only = False # if True, only show the schema of the feature store
		 ):
	""" Outputs the structure or contents of the feature store (intended for debugging/development) """
	if structure_only == True:
		with sqlite3.connect(feature_store.path) as conn:
			cursor = conn.cursor()
			cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
			tables = cursor.fetchall()
			for table in tables:
				print(table[0])
				cursor.execute(f"PRAGMA table_info({table[0]});")
				columns = cursor.fetchall()
				for column in columns:
					print(column[1:])
				print()
	else:
		with sqlite3.connect(self.path) as conn:
			table = []
			cursor = conn.execute('SELECT * FROM texts')
			for row in cursor:
				dump = {}
				dump['hash'] = row[0]
				dump['tokens'] = pickle.loads(row[1])
				dump['pos'] = pickle.loads(row[2])
				dump['textstats'] = pickle.loads(row[3])
				table.append(dump)

			print(f'Table: texts ({len(table)})')
			if len(table) > 0:
				display(pd.DataFrame(table))

			table = []
			cursor = conn.execute('SELECT * FROM embeddings')
			for row in cursor:
				dump = {}
				dump['hash'] = row[0]
				dump['embeddings'] = pickle.loads(row[1])
				table.append(dump)

			print(f'Table: embeddings ({len(table)})')
			if len(table) > 0:
				display(pd.DataFrame(table))

			table = []
			cursor = conn.execute('SELECT * FROM lexicons')
			for row in cursor:
				dump = {}
				dump['hash'] = row[0]
				dump['lexicons'] = pickle.loads(row[1])
				table.append(dump)

			print(f'Table: lexicons ({len(table)})')
			if len(table) > 0:
				display(pd.DataFrame(table))


# %% ../nbs/90_store.ipynb 12
@patch
def update(self:TextFeatureStore, 
			text:str, # the text to update
			tokens:list, # the tokens to update
			pos:list, # the part of speech tags to update
			textstats:list, # the text statistics to update
			):
	"""Update (insert or replce) the feature store with the token, parts of speech tag and textstat (document-level statistics) features for a specific text."""
	hash = hashlib.md5(text.encode()).hexdigest()
	with sqlite3.connect(self.path) as conn:
		conn.execute('''
			INSERT OR REPLACE INTO texts (hash, tokens, pos, textstats)
			VALUES (?, ?, ?, ?)
		''', (hash, pickle.dumps(tokens), pickle.dumps(pos), pickle.dumps(textstats)))
		conn.commit()

# %% ../nbs/90_store.ipynb 15
@patch
def update_embeddings(self:TextFeatureStore, 
						texts:str, # the texts to update 
						embeddings:list # the embeddings to update
						):
	"""Update the feature store with embeddings for a list of texts."""

	with sqlite3.connect(self.path) as conn:
		for i, text in enumerate(texts):
			hash = hashlib.md5(text.encode()).hexdigest()
			conn.execute('''
				INSERT OR REPLACE INTO embeddings (hash, embeddings) 
				VALUES (?, ?)
			''', (hash, pickle.dumps(embeddings[i])))
		conn.commit()


# %% ../nbs/90_store.ipynb 17
@patch
def update_lexicons(self:TextFeatureStore, 
					texts:str, # the texts to update 
					lexicons:list # the lexicon scores to update
					):
	"""Update the feature store with lexicon features for a list of texts."""

	with sqlite3.connect(self.path) as conn:
		for i, text in enumerate(texts):
			hash = hashlib.md5(text.encode()).hexdigest()
			conn.execute('''
				INSERT OR REPLACE INTO lexicons (hash, lexicons) 
				VALUES (?, ?)
			''', (hash, pickle.dumps(lexicons[i])))
			conn.commit()


# %% ../nbs/90_store.ipynb 19
@patch
def empty(self:TextFeatureStore):
	"""Clear the contents of the feature store."""
	with sqlite3.connect(self.path) as conn:
		conn.execute('''DELETE FROM texts;''')
		conn.execute('''DELETE FROM embeddings;''')
		conn.execute('''DELETE FROM lexicons;''')
		conn.commit()

# %% ../nbs/90_store.ipynb 21
@patch
def buffered_update(self:TextFeatureStore, 
			text:str, # the text to update
			tokens:list, # the tokens to update
			pos:list, # the part of speech tags to update
			textstats:list, # the text statistics to update
			):
	"""Update the feature store tokens, parts of speech tags and text statistics for multiple texts."""
	if not hasattr(self, 'buffer_'):
		self.buffer_ = []
	self.buffer_.append((text, tokens, pos, textstats))
	if len(self.buffer_) > 1000:
		self.flush()


# %% ../nbs/90_store.ipynb 22
@patch
def flush(self:TextFeatureStore):
	"""Flush the buffer to the database."""
	with sqlite3.connect(self.path) as conn:
		for text, tokens, pos, textstats in self.buffer_:
			# hash the text
			hash = hashlib.md5(text.encode()).hexdigest()
			conn.execute('''
				INSERT OR REPLACE INTO texts (hash, tokens, pos, textstats)
				VALUES (?, ?, ?, ?)
			''', (hash, pickle.dumps(tokens), pickle.dumps(pos), pickle.dumps(textstats)))
		conn.commit()
	self.buffer_ = []


# %% ../nbs/90_store.ipynb 24
@patch
def get(self:TextFeatureStore, 
		text:str, # the text to get features for
		type:str = None # the type of features to get - 'tokens', 'pos', 'textstats', 'embeddings', 'lexicons'
		) -> dict|list: # the features for the text
	""" Get features for a text."""

	if type is not None and type not in ['tokens', 'pos', 'textstats', 'embeddings', 'lexicons']:
		raise ValueError(f"Type {type} not in ['tokens', 'pos', 'textstats', 'embeddings', 'lexicons']")

	with sqlite3.connect(self.path) as conn:
		hash = hashlib.md5(text.encode()).hexdigest()
		if type is None:
			cursor = conn.execute('SELECT * FROM texts WHERE hash=?', (hash,))
			row = cursor.fetchone()
			result = {'tokens': pickle.loads(row[1]), 'pos': pickle.loads(row[2]), 'textstats': pickle.loads(row[3])}
			if row is None:
				return None
			else:
				cursor = conn.execute('SELECT * FROM embeddings WHERE hash=?', (hash,))
				row = cursor.fetchone()
				if row is None:
					result['embeddings'] = None
				else:
					result['embeddings'] = pickle.loads(row[1])

				cursor = conn.execute('SELECT * FROM lexicons WHERE hash=?', (hash,))
				row = cursor.fetchone()
				if row is None:
					result['lexicons'] = None
				else:
					result['lexicons'] = pickle.loads(row[1])
				return result

		elif type in ['tokens', 'pos', 'textstats']:
			cursor = conn.execute(f'SELECT {type} FROM texts WHERE hash=?', (hash,))
			row = cursor.fetchone()
			if row is None:
				return None
			else:
				return pickle.loads(row[0])
		elif type == 'embeddings':
			cursor = conn.execute('SELECT embeddings FROM embeddings WHERE hash=?', (hash,))
			row = cursor.fetchone()
			if row is None:
				return None
			else:
				return pickle.loads(row[0])
		elif type == 'lexicons':
			cursor = conn.execute('SELECT lexicons FROM lexicons WHERE hash=?', (hash,))
			row = cursor.fetchone()
			if row is None:
				return None
			else:
				return pickle.loads(row[0])


# %% ../nbs/90_store.ipynb 29
@patch
def get_features_from_texts_by_type(self:TextFeatureStore,
							texts:list, # the texts to get features for
							type:str # the type of features to get - 'tokens', 'pos', 'textstats', 'embeddings', 'lexicons'
							) -> list: # the features for the texts
	""" Get features for a list of texts by type, if no match returns None for the text."""
	hashes = [hashlib.md5(text.encode()).hexdigest() for text in texts]

	if type not in ['tokens', 'pos', 'textstats', 'embeddings', 'lexicons']:
		raise ValueError(f"Type {type} not in ['tokens', 'pos', 'textstats', 'embeddings', 'lexicons']")

	with sqlite3.connect(self.path) as conn:
		if type == 'embeddings':
			cursor = conn.execute(f'SELECT hash, embeddings FROM embeddings WHERE hash IN ({", ".join(["?"] * len(hashes))})', hashes)
		elif type == 'lexicons':
			cursor = conn.execute(f'SELECT hash, lexicons FROM lexicons WHERE hash IN ({", ".join(["?"] * len(hashes))})', hashes)
		else:
			cursor = conn.execute(f'SELECT hash, {type} FROM texts WHERE hash IN ({", ".join(["?"] * len(hashes))})', hashes)
		features = [None] * len(hashes)

		positions = {}
		for i, hash in enumerate(hashes):
			if hash not in positions:
				positions[hash] = []
			positions[hash].append(i)

		for row in cursor:
			if row[0] in hashes:
				for position in positions[row[0]]:
					features[position] = pickle.loads(row[1])
			else:
				raise ValueError(f"Hash {row[0]} not in {hashes}")
		
	return features


# %% ../nbs/90_store.ipynb 31
@patch
def get_tokens_from_texts(self:TextFeatureStore,
							texts:list, # the texts to get tokens for
							lowercase:bool = False, # whether to return tokens as lowercase
							min_token_length:int = 0, # the minimum token length to include
							remove_punctuation:bool = False, # whether to remove punctuation
							remove_numbers:bool = False, # whether to remove numbers
							) -> list: # the tokens for the texts
	""" Get (and optionally filter or transform) tokens for a list of texts. """
	tokens = self.get_features_from_texts_by_type(texts, 'tokens')
	if lowercase == True:
		tokens = [[token.lower() for token in text] if text is not None else None for text in tokens]
	if min_token_length > 0:
		tokens = [[token for token in text if len(token) >= min_token_length] if text is not None else None for text in tokens]
	if remove_punctuation == True:
		tokens = [[token for token in text if token.strip(string.punctuation)] if text is not None else None for text in tokens]
	if remove_numbers == True:
		tokens = [[token for token in text if not token.isdigit()] if text is not None else None for text in tokens]
	return tokens

# %% ../nbs/90_store.ipynb 33
@patch
def get_textstats_from_texts(self:TextFeatureStore,
								texts:list, # the texts to get text statistics for
								columns_out = ['tokens_count', 'sentences_count', 'characters_count', 'monosyllabic_words_relfreq', 'polysyllabic_words_relfreq', 'unique_tokens_relfreq', 'average_characters_per_token', 'average_tokens_per_sentence', 'characters_proportion_letters',  'characters_proportion_uppercase', 'hapax_legomena_count', 'hapax_legomena_to_unique'], # the columns to return
								columns_in = ['tokens_count', 'sentences_count', 'characters_count', 'monosyllabic_words_relfreq', 'polysyllabic_words_relfreq', 'unique_tokens_relfreq', 'average_characters_per_token', 'average_tokens_per_sentence', 'characters_proportion_letters',  'characters_proportion_uppercase', 'hapax_legomena_count', 'hapax_legomena_to_unique'], # the possible columns
								) -> list: # the text statistics for the texts
	""" Get document-level text statistics for a list of texts. """
	try:
		columns = [columns_in.index(col) for col in columns_out]
	except ValueError:
		raise ValueError(f"Columns {columns_out} not in {columns_in}")

	textstats = self.get_features_from_texts_by_type(texts, 'textstats')
	textstats = [[textstat[i] for i in columns] if textstat is not None else None for textstat in textstats]
	return textstats


# %% ../nbs/90_store.ipynb 36
@patch
def get_pos_from_texts(self:TextFeatureStore, 
						texts:list, # the texts to get part of speech tags for
						) -> list: # the part of speech tags for the texts
	""" Get parts of speech for a list of texts. """
	pos = self.get_features_from_texts_by_type(texts, 'pos')
	return pos


# %% ../nbs/90_store.ipynb 38
@patch
def get_embeddings_from_texts(self:TextFeatureStore,
								texts:str # the texts to get embeddings for
								) -> list: # the embeddings for the texts
	""" Get embeddings for multiple texts. """

	embeddings = self.get_features_from_texts_by_type(texts, 'embeddings')
	return embeddings


# %% ../nbs/90_store.ipynb 40
@patch
def get_lexicons_from_texts(self:TextFeatureStore,
							texts:str # the texts to get lexicons for
							) -> list:
	""" Get lexicon features for multiple texts. """
	lexicons = self.get_features_from_texts_by_type(texts, 'lexicons')
	return lexicons

