"""Preprocess text data."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/10_preprocess.ipynb.

# %% ../nbs/10_preprocess.ipynb 3
from __future__ import annotations
from sklearn.base import BaseEstimator, TransformerMixin
import spacy
from .store import TextFeatureStore
import textstat
from fastcore.basics import patch

# %% auto 0
__all__ = ['SpacyPreprocessor']

# %% ../nbs/10_preprocess.ipynb 4
try:
    nlp = spacy.load("en_core_web_sm")
except OSError:
    import spacy.cli
    spacy.cli.download("en_core_web_sm")


# %% ../nbs/10_preprocess.ipynb 5
class SpacyPreprocessor(BaseEstimator, TransformerMixin):
	""" A Sci-kit Learn pipeline component to preprocess text using spaCy, 
		the pipeline component receives and returns texts, but prepares tokens, pos, and text statistics 
		as input to other compatible classes in a pipeline. """
	def __init__(self, 
				feature_store: TextFeatureStore, # the feature store to use
				pos_tagset:str = 'simple', # 'simple' or 'detailed' (see note in documentation about the tag sets used)
				model_name:str = 'en_core_web_sm', # the spaCy model to use
				disable: list[str] = ['parser', 'ner'], # the spaCy components to disable
				enable: list[str] = ['sentencizer'], # the spaCy components to enable
				batch_size:int = 500, # the batch size for the Spacy processing
				n_process:int = 1 # the number of processes for Spacy to use
				 ):
		if pos_tagset in ['simple', 'detailed']:
			self.pos_tagset = pos_tagset
		else:
			raise ValueError('Invalid value for pos_tagset. Valid values are "simple" or "detailed".')
		self.model_name = model_name
		self.disable = disable
		self.enable = enable
		self.batch_size = batch_size
		self.n_process = n_process
		try:
			self.nlp = spacy.load(self.model_name, disable=self.disable)
			for component in self.enable:
				self.nlp.add_pipe(component)
		except OSError as e:
			raise OSError('Spacy model could not be loaded. The "en_core_web_sm" is downloaded by default. If you want to use another Spacy model you probably need to download it with "python -m spacy download your-model"') from e

		self.feature_store = feature_store


# %% ../nbs/10_preprocess.ipynb 7
@patch
def _iterator(self:SpacyPreprocessor, 
			X:list # the texts to iterate over
			):
	""" Iterator to yield texts one by one. """
	for text in X:
		if text is None:
			yield ''
		else:
			yield str(text)


# %% ../nbs/10_preprocess.ipynb 8
@patch
def _fit_textstats(self:SpacyPreprocessor, 
				doc:spacy.tokens.doc.Doc, # the spaCy document to fit text statistics for 
				tokens:list # the tokens to fit text statistics for
				) -> list: # the text statistics for the document
	""" Fit textstats for a document."""
	if len(tokens) == 0:
		return [0] * 12
	else:
		textstats = []
		tokens_count = len(tokens)
		sentence_count = len(list(doc.sents))
		character_count = textstat.char_count(doc.text, ignore_spaces=True)
		tokens_lower = [token.lower() for token in tokens]
		unique_tokens_count = len(set(tokens_lower))
		document_hapax_count = len(set(word for word in set(tokens_lower) if tokens_lower.count(word) == 1))
		
		textstats.append(tokens_count) # tokens count
		textstats.append(sentence_count) # sentences count
		textstats.append(character_count) # character count

		textstats.append(textstat.monosyllabcount(doc.text)/tokens_count) # monosyllablic words relative frequency
		textstats.append(textstat.polysyllabcount(doc.text)/tokens_count) # polysyllablic words relative frequency
		textstats.append(unique_tokens_count/tokens_count) # unique_tokens_count relative frequency or Type-Token Ratio (TTR)
		textstats.append(character_count/tokens_count) # average characters per token

		textstats.append(tokens_count/sentence_count) # average tokens per sentence

		textstats.append(textstat.letter_count(doc.text, ignore_spaces=True)/character_count) # proportion of all characters that are letters
		textstats.append((sum(1 for char in doc.text if char.isupper())) / character_count) # proportion of all characters that are uppercase letters

		textstats.append(document_hapax_count) # hapax legomena in document
		textstats.append(document_hapax_count / unique_tokens_count) # hapax legomena in document as proportion of unique tokens	
	return textstats

# %% ../nbs/10_preprocess.ipynb 9
@patch
def _spacy_tokenize(self:SpacyPreprocessor, 
					doc:spacy.tokens.doc.Doc # the spaCy document to tokenize
					) -> tuple[list[str], list[str]]: # returns lists of tokens and part of speech tags
	if self.pos_tagset == 'detailed':
		return [token.text for token in doc if not token.is_space], [token.tag_ for token in doc if not token.is_space]
	else:
		return [token.text for token in doc if not token.is_space], [token.pos_ for token in doc if not token.is_space]


# %% ../nbs/10_preprocess.ipynb 10
@patch
def fit(self:SpacyPreprocessor, X, y=None):
	""" Fit is implemented, but does nothing. """
	return self

# %% ../nbs/10_preprocess.ipynb 11
@patch
def transform(self:SpacyPreprocessor, X):
	""" Preprocess the texts using spaCy and populate the feature store ready for use later in a pipeline. """

	tokens = self.feature_store.get_tokens_from_texts(X)
	if any(x is None for x in tokens):
		for doc in self.nlp.pipe(self._iterator(X), batch_size=self.batch_size, n_process=self.n_process):
			tokens, pos = self._spacy_tokenize(doc)
			textstats = self._fit_textstats(doc, tokens)
			self.feature_store.buffered_update(doc.text, tokens, pos, textstats)
		self.feature_store.flush()				
	else:
		# all the tokens are already in the feature store so no need to reprocess
		pass
	return X
