"""
Classes with available solvers

A solver takes care of solving the (non-linear)
system of equations generated by the Simulation object.
This object can interact with the Simulation object.

The Simulation class expects the following methods
from the solver object:

- ``get_nb_steps_estimate()``
- ``run_step(simulation_step_index)``

Current solvers:
 1. :py:class:`.solver.NewtonConstantTimeStep`
 2. :py:class:`.solver.NewtonAdaptiveTimeStep02`

2020, July 23
"""

import numpy as np
from numpy.random import default_rng
import inspect


class BaseNewton:
    """
    Base class designed to ease creating new solvers
    by providing common solver functionality
    such as the Newton solver for solving systems of nonlinear equations.

    The discretization method is set with the parameter ``discretization``.
    The solver always solves the arguments for the selected timestep
    (in the method ``run_step``),
    and the solving is always done with the state values of that timestep,
    however the handling of the state differs.
    Most of the solvers in FONSim support several of the following
    three discretization methods (specified as a string):

    * **forward**: propagates the state to the next step.
    * **backward**: propagates the state from the previous to the current step
      with the arguments of the current step
    * **tustin**: propagates the state from the previous to the current step
      with the mean of the arguments of the previous
      and the current step

    :param simulation: Simulation object
    :param supported_discretization_methods: the discretization methods
                                             the inherited class supports,
                                             must be set by child class
    :param discretization: see above
    """
    def __init__(self, simulation, supported_discretization_methods,
                 discretization):
        self.sim = simulation

        # Whether allowed to run on a simulation step
        # that precedes the given step
        self.go_backwards_allowed = True

        # Discretization method
        if not isinstance(discretization, str):
            msg = f"The discretization method must be specified as a string, " \
                  f"not as a {str(type(discretization))}."
            raise ValueError(msg)
        if discretization not in supported_discretization_methods:
            msg = f"The selected discretization method '{str(discretization)}' " \
                  f"is not supported by this solver, please select " \
                  f"one of the following methods: " \
                  f"{supported_discretization_methods}."
            raise ValueError(msg)
        self.discretization = discretization

    def apply_solver_bias(self, bias=1e-12, step=0, quasirandom=False):
        """
        Give elements in the solution vector a small positive value, to
        ease starting the simulation

        If ``quasirandom=True``, the assigned biases
        come from a random number generator initialized with a constant seed
        (i.e. the bias array is the same on each method call).

        :param bias: bias value added to the solution vector entries
        :param step: simulation step for which this bias is applied
        :param quasirandom: whether to vary bias values
        :return: None
        """
        if quasirandom:
            rng = default_rng(seed=12345)
            bias *= (0.5 + 0.5*rng.random(self.sim.phi.shape[1]))
        self.sim.phi[step, :] += bias

    def get_all_variables(self, simstep):
        """
        :param simstep: simulation step at which variables are queried
        :return: np array with the values of all arguments and component
                 states at the desired simulation step
        """
        # Get all argument values
        arguments = self.sim.phi[simstep,:]
        # Get all state values
        states = self.sim.state[simstep, :]

        return np.append(arguments, states)

    def get_residual(self, simstep, res=None, dt=None,
                     discretization=None):
        """
        Evaluate the simulation system of equations at the provided
        timestep to get the residual vector.
        See the related method :py:meth:`.Simulation.evaluate_equations`
        for more documentation,
        as most arguments are passed to that method unmodified.

        :param simstep: index of simulation timestep at which the system
                        of equations is evaluated
        :param res: optional initialized residual vector that will be
                    overwritten by this function in place
        :param dt: timestep
        :param discretization: discretization method, gets passed to
                               :py:meth:`.Simulation.evaluate_equations`
        :return: evaluated residual vector
        """
        if res is None:
            res = np.zeros(self.sim.nb_arguments)
        elapsed_time = self.sim.times[simstep]

        # Evaluate equations and get updated jacobian H
        self.sim.evaluate_equations(simstep, res, self.sim.H,
                                    elapsed_time, self.dt,
                                    discretization)
        # evaluate network equations (can this be done more elegantly?)
        res[:self.sim.nb_network_equations] = np.dot(self.sim.A,
                                                     self.sim.phi[simstep,:])

        return res

    def newton_solver(self, step, iterations=100, alpha=1.,
                      discretization=None, min_nb_iterations=1):
        """
        Newton method for solving the system equations and storing the
        solution in the simulation phi vector at an time step

        :param step: step index for which the system will be solved and
                     the solution will be stored. The initialization for the solver
                     can be done externally by setting self.sim.phi[step] to the
                     initial guess
        :param iterations: maximum number of newton solver iterations.
                           100 (default) gives good results although it often converges in
                           one step and otherwise mostly in less than ten
        :param alpha: correction constant for damped Newton method
        :param discretization: discretization method, gets passed to
                               :py:meth:`.solver.BaseNewton.get_residual`
        :return: flag indicating exit status of the solver for this step:

                 * 0: maximum amount of iterations reached without convergence
                 * 1: solver converged quickly
                 * 2: solver converged slowly
        """
        # Allocate residual vector
        residual = np.zeros(self.sim.nb_arguments)
        # Newton outer loop
        for i in range(iterations):
            # Get the residual vector
            self.get_residual(step, residual,
                              discretization=discretization)
            # Look at error
            # residual is scaled with inverse of equation solving tolerances,
            # aka when the scaled residual is smaller than 1.0,
            # the desired error has been attained.
            # The equation tolerances are updated every outer loop iteration
            # because some components swap equations between iterations
            # and thus the corresponding equation residuals change considerably.
            if True:
                self.sim.fill_component_equation_tolerances(
                    self.sim.equation_tolerances, self.sim.equation_tolerances_inv)
            residual_scaled = self.sim.equation_tolerances_inv * residual
            max_error = np.max(np.abs(residual_scaled))
            # See if the error is sufficiently small
            # note: if the maximum error is sufficiently small
            # already in the first iteration of this (inner) loop,
            # no linear system solving is done
            # and the argument values in this simulation step
            # are kept the same as in the previous step
            # (assuming they were initialized with the values
            # from the previous step).
            # Use 0.4 to avoid triggering
            # the timestep sizing interpolation check.
            if self.sim.verbose:
                print(f'    max_error: {max_error:.1f}')
            if np.isnan(max_error):
                break
            if max_error < 0.4 and i > min_nb_iterations:
                #print("Outer loop converged in ", i + 1, "steps")
                if i <= np.ceil(.1*iterations):
                    return 1
                else:
                    return 2
            # So the error is larger than desired
            # Solve the linear system
            try:
                phi_correction = -np.linalg.solve(self.sim.H, residual)
            except np.linalg.LinAlgError as e:
                # probably singular matrix or so
                if self.sim.verbose: print(e)
                return 0
            # Scale down the correction if it is so large
            # that it would push any arguments outside their allowed range
            # todo: rewrite this to be more elegant and faster
            max_correction_pos = self.sim.phi_range[:, 0] - self.sim.phi[step, :]
            max_correction_neg = self.sim.phi_range[:, 1] - self.sim.phi[step, :]
            mask_pos = max_correction_pos != 0
            mask_neg = max_correction_neg != 0
            rel_correction = np.zeros((self.sim.nb_arguments, 2), dtype=float)
            rel_correction[mask_pos, 0] = phi_correction[mask_pos] / max_correction_pos[mask_pos]
            rel_correction[mask_neg, 1] = phi_correction[mask_neg] / max_correction_neg[mask_neg]
            times_too_large = max(np.amax(rel_correction[:, 0]), np.amax(rel_correction[:, 1]))
            if alpha * times_too_large > 1:
                # take 0.9 to keep a little distance from the limits
                phi_correction *= 0.9 / times_too_large
            # Apply the correction
            self.sim.phi[step,:] = self.sim.phi[step,:] + alpha*phi_correction
        if self.sim.verbose:
            print(f'Exited without convergence with max_error {max_error:.1f}')
        return 0

    def run_forward(self, step):
        """Run forward solver on a single step"""
        phi_old = self.sim.phi[step, :].copy()
        if self.sim.verbose >= 1: print(f'run_forward simstep {step}')
        status = self.newton_solver(step, discretization='forward')
        if status == 0 and self.sim.verbose >= 1:
            self.print_report(step)
        if self.sim.verbose >= 1: print(f'end run_forward simstep {step}')
        # If did not converge, reset original values
        if status == 0:
            self.sim.phi[step, :] = phi_old

    def print_report(self, simstep):
        """
        Print a report on the solver status at a certain simulation time
        step.

        :param simstep: index of simulation timestep for which the report
                        is generated
        :return: None
        """
        # display simulation time
        print("t = {}".format(self.sim.times[simstep]))
        # display argument values
        arg_str = ""
        for i in range(self.sim.nb_arguments):
            if i > 0:
                arg_str += ", "
            arg_str += "{} = {}".format(self.sim.arguments[i].short_str(),
                                        self.sim.phi[simstep,i])
        print(arg_str)

        # display residuals of non-converging equations
        residuals = self.get_residual(simstep)
        for i in range(len(residuals)):
            if abs(residuals[i]) > 0.1:
                print("eq {}: {} = {}".format(i, 
                      self.sim.equation_to_string(i), residuals[i]))
        # display system jacobian
        with np.printoptions(linewidth=300, formatter={'float': lambda f: f'{f:6.0f}'}) as opts:
            print("B:\n", self.sim.H[self.sim.nb_network_equations:, :])

        print()


class NewtonConstantTimeStep(BaseNewton):
    """
    Supported discretization methods:
    'forward', 'backward' and 'tustin'.

    Please see the base class for more information.

    :param simulation: Simulation object
    :param step: fixed time increment
    :param discretization: discretization method
    """
    def __init__(self, simulation, step, discretization):
        super().__init__(simulation, supported_discretization_methods={
            'forward', 'backward', 'tustin',
        }, discretization=discretization)

        # stepping
        self.dt = step

    def get_nb_steps_estimate(self):
        """
        :return: number of timesteps the solver needs
        """
        return int(np.ceil(self.sim.duration / self.dt)) + 1

    def run_step(self, simstep):
        """
        Run a single step of the solver.
        Depending on the selected discretization
        (``self.discretization``), different timestep indices
        are used and affected (please see class doc).

        :param simstep: index of simulation timestep with the last results
        :return: None
        """
        if self.sim.verbose >= 1:
            padding = '|   ' * (len(inspect.stack(0)) - 3)
            print(padding + 'solver.run_step(simstep), simstep:', simstep)

        # Solve arguments of first step, always using forward discretization
        # (because no previous state is available)
        if simstep == 0:
            self.sim.times[0] = 0
            self.apply_solver_bias(step=0, quasirandom=True)
            self.run_forward(0)
        # simstep > 0
        else:
            # Update the time where the propagated state will land
            self.sim.times[simstep] = self.sim.times[simstep-1] + self.dt

            # Propagate the state
            if self.discretization == 'forward':
                self.sim.update_state(simstep_args=simstep-1, simstep_state=simstep-1,
                                      dt=self.dt, discretization=self.discretization)

            # First estimate for new arguments: arguments of previous step
            self.sim.phi[simstep, :] = self.sim.phi[simstep-1, :]
            # Solve system for arguments
            status = self.newton_solver(simstep, discretization=self.discretization)
            # Report on solver convergence
            if status == 0:
                # Report on solver convergence
                print(f'Solver failed to converge at step {simstep}')
                if self.sim.verbose >= 1: self.print_report(simstep)

            # Update the state with the new arguments
            if self.discretization in ('backward', 'tustin'):
                self.sim.update_state(simstep_args=simstep, simstep_state=simstep-1,
                                      dt=self.dt, discretization=self.discretization)


class NewtonAdaptiveTimeStep02(BaseNewton):
    """
    Supported discretization methods:
    'backward' and 'tustin'.
    'forward' is not yet supported.

    Please see base class for more information.

    :param simulation: Simulation object
    :param step: minimum and maximum time increment
    :param discretization: discretization method
    """
    def __init__(self, simulation, step, discretization):
        super().__init__(simulation, supported_discretization_methods={
            'backward', 'tustin',
        }, discretization=discretization)

        # stepping
        self.dt_min = step[0]
        self.dt_max = step[1]

        self.dt = self.dt_min

        # Magic numbers for adjusting timestep
        self.factor_decrease = 0.5
        self.factor_increase = 1.5
        self.threshold_increase = 0.5

    def get_nb_steps_estimate(self):
        """
        :return: number of timesteps the solver needs
        """
        return int(np.ceil(self.sim.duration / self.dt_min)) + 1

    def run_step(self, simstep, recursion_allowed=True):
        """
        Run a single step of the solver.

        :param simstep: index of simulation timestep with the last results
        :param recursion_allowed: whether it may call itself
        :return: None
        """
        if self.sim.verbose >= 1:
            padding = '|   ' * (len(inspect.stack(0)) - 3)
            print(padding + 'solver.run_step(simstep), simstep:', simstep)

        # Solve arguments of first step
        if simstep == 0:
            self.sim.times[0] = 0
            self.apply_solver_bias(step=0, quasirandom=True)
            self.run_forward(0)
        else:
            # Update current time
            self.sim.times[simstep] = self.sim.times[simstep-1] + self.dt
            # First estimate for new arguments: arguments of previous step
            self.sim.phi[simstep, :] = self.sim.phi[simstep-1, :]

            if self.sim.verbose >= 1:
                print(padding + '| dt  [ms]:', '{:7.2f}'.format(self.dt*1000))
                print(padding + '| t   [ms]:',
                      '{:7.2f}'.format(self.sim.times[simstep-1] * 1e3) +
                      ' -> {:7.2f}'.format(self.sim.times[simstep] * 1e3))

            # Solve system for arguments
            status = self.newton_solver(simstep, discretization=self.discretization)
            # Solver did not converge and we're already at the smallest timestep
            if status == 0 and self.dt == self.dt_min:
                # Report on solver convergence
                print(f'Solver failed to converge at step {simstep}')
                if self.sim.verbose >= 1: self.print_report(simstep)

            # update the state with the updated arguments
            self.sim.update_state(simstep_args=simstep, simstep_state=simstep-1,
                                  dt=self.dt, discretization=self.discretization)

        # Look whether timestep has to be adjusted
        # and do recursive call if timestep decreased.
        # Don't adjust it during the first five steps
        # as then tolerances not yet well initialized.
        if simstep < 5 or recursion_allowed is False:
            pass
        else:
            # Check whether timestep sufficiently small
            # by comparing the derived argument and state vectors at step `simstep`
            # with the one interpolated between `simstep - 1` and `simstep + 1`.
            t = self.sim.times
            rico_arg_t = (self.sim.phi[simstep] - self.sim.phi[simstep-2]) / (t[simstep] - t[simstep-2])
            phi_ip = self.sim.phi[simstep-2] + rico_arg_t * (t[simstep-1] - t[simstep-2])
            rico_state_t = (self.sim.state[simstep] - self.sim.state[simstep-2]) / (t[simstep] - t[simstep-2])
            state_ip = self.sim.state[simstep-2] + rico_state_t * (t[simstep-1] - t[simstep-2])
            na, ns = self.sim.nb_arguments, self.sim.nb_states
            interpolation_err = np.zeros(na+ns, dtype=float)
            # also multiply the error with inverse of relative tolerances
            # and take maximum absolute value
            interpolation_err[:na] = (self.sim.phi[simstep] - phi_ip) * self.sim.argument_tolerances_inv
            interpolation_err[na:na+ns] = (self.sim.state[simstep] - state_ip) * self.sim.state_tolerances_inv
            np.abs(interpolation_err, out=interpolation_err)
            max_error = np.max(interpolation_err)
            if self.sim.verbose:
                print(padding + '| max_error:', '{:5.1f}'.format(max_error))
            # If too large, go back two steps and retry
            # also do so on non-convergence
            if (max_error > 1 or status == 0) and self.dt > self.dt_min:
                # Reduce timestep
                self.dt = max(self.dt*self.factor_decrease, self.dt_min)
                # and run this step again, as well as the previous step
                # (recursive calls)
                if self.go_backwards_allowed:
                    self.run_step(simstep-1, recursion_allowed=False)
                self.run_step(simstep, recursion_allowed=self.go_backwards_allowed)
            else:
                # If error small, increase timestep
                if max_error < self.threshold_increase:
                    self.dt = min(self.dt*self.factor_increase, self.dt_max)
