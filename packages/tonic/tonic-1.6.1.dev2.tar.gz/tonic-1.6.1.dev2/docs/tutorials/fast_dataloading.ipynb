{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2294504a-8816-412e-a590-be4c941acdc9",
   "metadata": {},
   "source": [
    "# Fast data loading with Tonic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904ec1e0-73fc-4521-9638-92dcd579fa8d",
   "metadata": {},
   "source": [
    "When training spiking neural networks, we typically experience long training times, depending on the number of time steps and training algorithm used. One thing that should not contribute to long training times is the time it takes to load a potentially transformed sample. For a start, let's measure the time it takes to apply a transform to 100 NMNIST samples without any tricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6bacf8-7965-41b6-9b24-54fbf783f2b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tonic\n",
    "import tonic.transforms as transforms\n",
    "\n",
    "sensor_size = tonic.datasets.NMNIST.sensor_size\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Denoise(filter_time=10000),\n",
    "        transforms.ToFrame(sensor_size=sensor_size, n_time_bins=3),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = tonic.datasets.NMNIST(save_to=\"./data\", train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cfe6a1-1aaa-45fc-b6ec-14e1214e07d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample_simple():\n",
    "    for i in range(100):\n",
    "        events, target = dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06fbb49-c3bb-4a44-ab49-bcbc58cfe43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -o load_sample_simple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb8c64a-9c13-47e4-84ed-f899f39742c4",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Loading time for 60k samples and 200 epochs: ~{int(_.average*600*200/3600)} minutes.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc661a6e-5432-4ef8-8ee9-e612f93b01bd",
   "metadata": {},
   "source": [
    "## Dataloaders with multithreading support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9390f9-e9ec-4adc-a906-43586811bbf7",
   "metadata": {},
   "source": [
    "To speed up things a bit, we can make use of sophisticated dataloaders, which provide support for pre-fetching data, multiple worker threads, batching and other things. Let's try the PyTorch dataloader. You can find all the supported functionality [in the official documentation](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef6d5d7-f49a-4c6e-9032-bb042dcdb3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, num_workers=2, shuffle=True)\n",
    "\n",
    "\n",
    "def load_sample_pytorch():\n",
    "    for i, (events, target) in enumerate(iter(dataloader)):\n",
    "        if i > 99:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ebd4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_sample_pytorch = lambda: next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74f887e-7117-44df-9f6e-702bf065cdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit load_sample_pytorch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25285388-0af1-4353-b452-fcc9f204d8c2",
   "metadata": {},
   "source": [
    "## Dataset Caching\n",
    "\n",
    "Even with a smarter Dataloader, we still do 2 things:\n",
    "\n",
    "1. We mostly read from files that are slow to read, maybe because they are in an inefficient binary format or just optimized for disk space.\n",
    "2. We apply our deterministic transform every time, for each epoch again. When working with events, we often want to preprocess them into a format that's more suitable for current accelerators. There's no need to do that multiple times, since the preprocessing will be deterministic, meaning it will lead to the same result given the same input and transform. \n",
    "\n",
    "To address these two issues, Tonic provides a `DiskCachedDataset`. A `DiskCachedDataset` wraps around your dataset object of choice. Whenever you load a sample, it applies the original transforms to your data and saves the result on disk in an efficient and convenient format. The next time you want to read the same sample, we will just read from that new file instead.\n",
    "In practice, this means that while your first epoch might be similarly slow as before, the following epochs will load much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c6a0ce-0434-4334-b5fc-d1e0512f6c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tonic import DiskCachedDataset\n",
    "\n",
    "cached_dataset = DiskCachedDataset(dataset, cache_path=\"./cache/fast_dataloading\")\n",
    "cached_dataloader = DataLoader(cached_dataset, num_workers=2)\n",
    "\n",
    "\n",
    "def load_sample_cached():\n",
    "    for i, (events, target) in enumerate(iter(cached_dataloader)):\n",
    "        if i > 99:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aed9ab-75f7-465d-b0e7-99b382fbdafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -o -r 20 load_sample_cached()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085c8cae-6027-415d-a35e-a99548d6259a",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Loading time for 60k samples and 200 epochs with cache: ~{int(_.average*600*200/3600)} minutes.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98c7c83-7a1b-4b11-bbed-06351e210580",
   "metadata": {},
   "source": [
    "### Augmentations on top of disk-cached data\n",
    "If we want to apply stochastic transformations as well, we can pass another set of transforms to the DiskCachedDataset, which will then apply them after reading them from the cache. In the following example, we will convert our cached samples (which are already frames) to tensors and then apply random rotations to the whole recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc67185-59ab-43e8-bdbb-d1959837e6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "transform = tonic.transforms.Compose(\n",
    "    [torch.tensor, torchvision.transforms.RandomRotation([-30, 30])]\n",
    ")\n",
    "augmented_dataset = DiskCachedDataset(\n",
    "    dataset, cache_path=\"./cache/fast_dataloading2\", transform=transform\n",
    ")\n",
    "augmented_dataloader = DataLoader(augmented_dataset, num_workers=2)\n",
    "\n",
    "\n",
    "def load_sample_augmented():\n",
    "    for i, (events, target) in enumerate(iter(augmented_dataloader)):\n",
    "        if i > 99:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8d542a-ceda-4de8-b303-9a9a3e39c729",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -r 20 load_sample_augmented()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "65b6f4b806bbaf5b54d6ccaa27abf7e5307b1f0e4411e9da36d5256169cebdd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
