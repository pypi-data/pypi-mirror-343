{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccd63c76-ef17-4afd-b6ce-e839c123b086",
   "metadata": {},
   "source": [
    "# Working with larger datasets and multiple data\n",
    "Some datasets contain GB of multiple data types and targets. In this tutorial we are going to look at how we can slice different data types at the same time in long recordings  into smaller chunks and how we can cache those slices to disk for efficient loading. As an example we are going to work with the DAVIS dataset. It's convenient since we can pick and download single recordings of a few hundred MB in size for the purpose of this tutorial but the lessons learned will scale to larger datasets as well. One recording contains a tuple of data for (events, imu, images). Let's start by downloading it. This tutorial also works with the Visual Place Recognition dataset (VPR), but be aware that it's much larger at ~74 GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b497641-bbd6-45b1-af91-8c85729c640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tonic\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dc5b07-4144-405d-9538-b07dd00849cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tonic.datasets.DAVISDATA(\n",
    "    save_to=\"./data\", recording=[\"shapes_6dof\", \"shapes_rotation\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a669a499-2a82-4fcb-a0eb-3671a4a8fdab",
   "metadata": {},
   "source": [
    "Not only do we want to slice the events for this recording, we also want to slice imu and image data at the same time steps. For that we'll have to write a custom slicing method which implements the tonic.slicers.Slicer protocol. That means that we need to implement at least `get_slice_metadata` and `slice_with_metadata` without having to subclass it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07b9457-8bd8-4ded-b959-e2afa26b378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tonic.slicers import SliceByTime\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, List, Tuple\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MultiDataSlicer:\n",
    "    time_window: float\n",
    "    overlap: float = 0.0\n",
    "    include_incomplete: bool = False\n",
    "\n",
    "    # this method receives all the data for one recording/sample.\n",
    "    # Based on the timestamps in there, we'll work out the boundaries\n",
    "    # of slices, in this case according to a time window. This method\n",
    "    # is called once per sample.\n",
    "    def get_slice_metadata(self, data, targets):\n",
    "        events, imu, images = data\n",
    "        min_ts = min(min(events[\"t\"]), min(imu[\"ts\"]), min(images[\"ts\"]))\n",
    "        max_ts = max(max(events[\"t\"]), max(imu[\"ts\"]), max(images[\"ts\"]))\n",
    "        stride = self.time_window - self.overlap\n",
    "        if self.include_incomplete:\n",
    "            n_slices = int(np.ceil(((max_ts - min_ts) - self.time_window) / stride) + 1)\n",
    "        else:\n",
    "            n_slices = int(\n",
    "                np.floor(((max_ts - min_ts) - self.time_window) / stride) + 1\n",
    "            )\n",
    "        window_start_times = np.arange(n_slices) * stride + min_ts\n",
    "        window_end_times = window_start_times + self.time_window\n",
    "        return list(zip(window_start_times, window_end_times))\n",
    "\n",
    "    # Even if we are only interested in a single slice, the data is still stored in a file for the\n",
    "    # whole recording. To access that slice, we thus need to load the whole recording and then pick\n",
    "    # the part of it that we are interested in. This method receives the whole data recording and\n",
    "    # metadata about where a slice starts and stops. This can be timestamps, indices or other things.\n",
    "    # In this example we just copy the targets for each new slice by passing them along.\n",
    "    @staticmethod\n",
    "    def slice_with_metadata(data: Tuple[Any], targets: Tuple[Any], metadata: List[Tuple[int, int]]):\n",
    "        events, imu, images = data  # this is data for a whole recording\n",
    "        start, stop = metadata[0][0], metadata[0][1]\n",
    "        event_slice = events[np.logical_and(events[\"t\"] >= start, events[\"t\"] < stop)]\n",
    "        imu_slice = {}\n",
    "        imu_slice[\"ts\"] = imu[\"ts\"][\n",
    "            np.logical_and(imu[\"ts\"] >= start, imu[\"ts\"] < stop)\n",
    "        ]\n",
    "        imu_slice[\"rotQ\"] = imu[\"rotQ\"][\n",
    "            np.logical_and(imu[\"ts\"] >= start, imu[\"ts\"] < stop)\n",
    "        ]\n",
    "        imu_slice[\"angV\"] = imu[\"angV\"][\n",
    "            np.logical_and(imu[\"ts\"] >= start, imu[\"ts\"] < stop)\n",
    "        ]\n",
    "        imu_slice[\"acc\"] = imu[\"acc\"][\n",
    "            np.logical_and(imu[\"ts\"] >= start, imu[\"ts\"] < stop)\n",
    "        ]\n",
    "        imu_slice[\"mag\"] = imu[\"mag\"][\n",
    "            np.logical_and(imu[\"ts\"] >= start, imu[\"ts\"] < stop)\n",
    "        ]\n",
    "        image_slice = {}\n",
    "        image_slice[\"ts\"] = images[\"ts\"][\n",
    "            np.logical_and(images[\"ts\"] >= start, images[\"ts\"] < stop)\n",
    "        ]\n",
    "        image_slice[\"frames\"] = images[\"frames\"][\n",
    "            np.logical_and(images[\"ts\"] >= start, images[\"ts\"] < stop)\n",
    "        ]\n",
    "        return (event_slice, imu_slice, image_slice), targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391ecea9-e457-467b-9e3a-ec9516c80e39",
   "metadata": {},
   "source": [
    "Now that we specified how our recording should be chunked, we'll wrap our dataset in a SlicedDataset class, where we pass our MultiDataSlicer object. To showcase a common use case, we'll also specify a ToFrame transform which will be applied to every slice after loading it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff3c1de-88da-4bf9-9e0b-621bc5e541c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tonic import SlicedDataset\n",
    "import tonic.transforms as transforms\n",
    "\n",
    "# the time length of one slice of recording\n",
    "slicing_time_window = 200000\n",
    "slicer = MultiDataSlicer(time_window=slicing_time_window)\n",
    "\n",
    "# bin events in a slice to frames\n",
    "frame_transform = transforms.ToFrame(sensor_size=dataset.sensor_size, time_window=2000)\n",
    "\n",
    "\n",
    "def custom_transform(data):\n",
    "    events, imu, images = data\n",
    "    return (frame_transform(events), imu, images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de2e98b-4a99-419f-84f8-09b0ebd6f8e0",
   "metadata": {},
   "source": [
    "Because it is quite expensive to compute the metadata for a large dataset, we'll also provide a path where it is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa92dc3-e39e-4512-b317-36427472cc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d7c617-c0ed-49ad-a0c4-294246567de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sliced_dataset = SlicedDataset(\n",
    "    dataset,\n",
    "    slicer=slicer,\n",
    "    transform=custom_transform,\n",
    "    metadata_path=\"./metadata/large_datasets\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56284f91-4525-4d55-8fb0-939eb15f9acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cut a dataset of {len(dataset)} recording into {len(sliced_dataset)} slices.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c5acca-b129-4f6c-ae55-cf2f620790d3",
   "metadata": {},
   "source": [
    "The next time we instantiate this SlicedDataset, we'll just load it from disk for a considerable speed up of accessing slicing metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cabd24-c65b-47d1-9801-c689c4073861",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sliced_dataset = SlicedDataset(\n",
    "    dataset,\n",
    "    slicer=slicer,\n",
    "    transform=custom_transform,\n",
    "    metadata_path=\"./metadata/large_datasets\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff3c473-be24-487f-9d0e-8b90ead8390c",
   "metadata": {},
   "source": [
    "In a last step, we are going to make use of caching. This is important to avoid loading the whole recording whenever we want to load a slice. When we wrap our sliced dataset in a CachedDataset, we write the data for one slice to disk. Next time want that same slice, we can just load it from disk, where it sits in an efficient format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55a7c3f-d421-4937-8042-90eec17b5065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tonic import SlicedDataset, CachedDataset\n",
    "\n",
    "cached_dataset = CachedDataset(sliced_dataset, cache_path=\"./cache/large_datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e873092-984e-4de5-973a-19c9769c5128",
   "metadata": {},
   "source": [
    "The first time we access a sliced sample, under the hood Tonic loads the whole recording, slices it according to metadata, applies transforms and eventually writes it to a cache directory before returning the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d2ac0e-2fa6-4ac0-ac52-9f0da3451b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# first time access\n",
    "(event_frames, imu, images), targets = cached_dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfefae0b-4657-4cc4-86b8-e91bd4f6a7dd",
   "metadata": {},
   "source": [
    "Let's verify that the data looks alright:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f77f38b-4e2a-40ed-9a13-57f7789f1b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Event frames have a shape of {event_frames.shape},\\nimages for this slice have a shape of {images['frames'].shape} and go from {images['ts'][0]} to {images['ts'][-1]} microseconds\\nand imu time stamps range from {imu['ts'][0]} to {imu['ts'][-1]} microseconds.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b8771c-61c9-4f0b-ac0e-1a07ee88e875",
   "metadata": {},
   "source": [
    "Next time we access this particular sample, it will be faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f522b0-4576-4e80-8711-f78ea59056c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# second time access\n",
    "(event_frames, imu, images), targets = cached_dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544ac69a-1205-4870-8ce4-2275f11c25bc",
   "metadata": {},
   "source": [
    "Last but not least we also check the disk footprint of a single slice in cache. During caching, we make use of lightweight lzf compression, which can save a lot of space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1889d7e6-5aec-4c05-ad50-34b065883a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "print(\n",
    "    f\"Last slice takes {sum(p.stat().st_size for p in Path('./cache/large_datasets').rglob('*'))/1e6} MB on disk.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc88605-6148-434c-a311-d10f64c1c507",
   "metadata": {},
   "source": [
    "That's pretty good for some 100 images, plus imu and gps data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "caf264bf03997fa53b380c84044763293a7a6f8ebb5555ee5243fd4d1f495be6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
