{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72ab4fec-4fe8-49cc-867f-6a77bcfaa062",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Loading the event-based version of MNIST\n",
    "\n",
    "With Tonic you can do 2 major things:\n",
    "\n",
    "1. Load event-based datasets\n",
    "2. Add transforms to the dataset that are automatically applied every time you load a new sample.\n",
    "\n",
    "\n",
    "Loading data\n",
    "------------\n",
    "\n",
    "Let's say you would like to work on N-MNIST, the neuromorphic equivalent of the popular MNIST dataset. In this dataset, a digit is displayed on screen and a neuromorphic camera in front of it executes 3 rapid sweeps in a triangular motion, as otherwise static images are not recorded with such cameras. The following animation is taken from [this repo](https://github.com/rfma23/HATS).\n",
    "\n",
    "\n",
    "[<img src=\"https://raw.githubusercontent.com/rfma23/HATS/master/images/nmnist.gif\" width=\"250\"/> ](https://raw.githubusercontent.com/rfma23/HATS/master/images/nmnist.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8e4e07-be9b-45eb-b7f9-de070adbaa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tonic\n",
    "\n",
    "dataset = tonic.datasets.NMNIST(save_to=\"../tutorials/data\", train=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e9f45be-20da-44f8-962b-2b842fc862df",
   "metadata": {},
   "source": [
    "Now you can index the samples manually for inspection! Depending on the dataset, you can expect different return values. N-MNIST returns events and the target class for each sample. Other datasets provide images, IMU data, GPS data and more. You will find the details in the dataset reference. Events are a structured numpy array with different channels. For recordings from event cameras, those channels are typically x, y, time and polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8c390a-3e50-4e54-b53a-3c1bc35277a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "events, target = dataset[1000]\n",
    "events"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dba2d3f0-f82b-4f16-a06f-97669b73bec4",
   "metadata": {},
   "source": [
    "Event timestamps in Tonic will always have microsecond resolution. We can accumulate/bin many events into a frame to visualise them. That's what we're going to do next.\n",
    "\n",
    "Applying a transform manually\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f2b5fc-f157-4ee3-83a7-779f78dfa6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tonic.transforms as transforms\n",
    "\n",
    "sensor_size = tonic.datasets.NMNIST.sensor_size\n",
    "frame_transform = transforms.ToFrame(sensor_size=sensor_size, n_time_bins=3)\n",
    "\n",
    "frames = frame_transform(events)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dcfd768d-0a7f-48c8-b1d1-6f80d0b516a4",
   "metadata": {},
   "source": [
    "The frames have dimensions (Time, Number of polarities, Height and Width). Let's plot one frame for each of the three saccades in a sample of N-MNIST. We'll take the difference between two camera polarities to see the direction of movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1c2842-a7e4-4839-b63e-f4d41264f5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_frames(frames):\n",
    "    fig, axes = plt.subplots(1, len(frames))\n",
    "    for axis, frame in zip(axes, frames):\n",
    "        axis.imshow(frame[1] - frame[0])\n",
    "        axis.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "plot_frames(frames)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9733a1da-96a5-4ab1-ad0e-d0d46f7a305c",
   "metadata": {},
   "source": [
    "You can see nicely the three saccades for this sample. The bright and dark version of the digit is because an event camera outputs two polarities, one for ON events that signify an increase in illuminance, and one for OFF events that signify a decrease.\n",
    "\n",
    "In the previous plot we can see some isolated noise events, let's try to get rid of them. We'll use a transform that deletes such isolated events, and then apply it to our events. Notice the order we're applying. Then we are going to plot the denoised frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aa324e-79b7-497d-9cf5-ada4c2fd0e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "denoise_transform = tonic.transforms.Denoise(filter_time=10000)\n",
    "\n",
    "events_denoised = denoise_transform(events)\n",
    "frames_denoised = frame_transform(events_denoised)\n",
    "\n",
    "plot_frames(frames_denoised)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10406fbc-ddb2-4104-9a0d-ab375f3e653c",
   "metadata": {
    "tags": []
   },
   "source": [
    "That looks a bit cleaner!\n",
    "\n",
    "Converting to different representation\n",
    "--------------------------------------\n",
    "Conversion to frames is not the only representation that events can be transformed into. As an example, we can also plot voxel grids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1181194d-3de4-4f83-ad22-f39f0197bbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = transforms.ToVoxelGrid(sensor_size=sensor_size, n_time_bins=3)(events_denoised)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(volume))\n",
    "for axis, slice in zip(axes, volume):\n",
    "    axis.imshow(slice[0])\n",
    "    axis.axis(\"off\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c9d5bb7-8658-409a-bcc7-24c7e0587479",
   "metadata": {},
   "source": [
    "Or time surfaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb332e9-fc69-428c-bf20-2b71fed89edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "surfaces = transforms.ToTimesurface(sensor_size=sensor_size, dt=99000, tau=100000)(events_denoised)\n",
    "\n",
    "n_events = events_denoised.shape[0]\n",
    "n_events_per_slice = n_events // 3\n",
    "fig, axes = plt.subplots(1, 3)\n",
    "for i, axis in enumerate(axes):\n",
    "    surf = surfaces[i]\n",
    "    axis.imshow(surf[0] - surf[1])\n",
    "    axis.axis(\"off\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a167e44a-39d8-4e42-b5e5-39a9e9179463",
   "metadata": {},
   "source": [
    "Putting it all together\n",
    "-----------------------\n",
    "Previously we applied the transformations manually. We can simplify this code, by chaining the transforms and passing them to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817fd0fc-0942-4725-8135-1aa3044da444",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([denoise_transform, frame_transform])\n",
    "\n",
    "dataset = tonic.datasets.NMNIST(\n",
    "    save_to=\"../tutorials/data\", train=False, transform=transform\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2f027cc-55ba-46b6-848d-deedf4e5a3e4",
   "metadata": {},
   "source": [
    "Now the transforms will be applied whenever a new sample is loaded. To simplify the loading, we make use of a PyTorch DataLoader in a final step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62099ec2-21fa-47f9-86ee-68a0534f9629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, shuffle=True)\n",
    "frames, target = next(iter(dataloader))\n",
    "\n",
    "plot_frames(frames.squeeze())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aac04620-cf13-4c03-b653-7d3520d52380",
   "metadata": {},
   "source": [
    "And that's there is to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35013a2a-1083-4bf2-bf62-b06faeed94c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "65b6f4b806bbaf5b54d6ccaa27abf7e5307b1f0e4411e9da36d5256169cebdd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
