"""Loads requested subtask for SolarCube.

"""
import gc
import os
import h5py
import pandas as pd
import numpy as np
from typing import Dict, Any, Tuple, Union, List
from concurrent.futures import ThreadPoolExecutor, as_completed

# number of meteorological stations and geographic areas.
NUMBER_STATIONS = 19

# past and future time window of 24h in 15 minute steps equals 96 time steps.
TIME_WINDOW_STEPS = 96

# The standardized split ratio for the entire dataset: train, val, test
SPLIT_RATIO = (0.5, 0.1, 0.4)

# list of satellite data available to load
SAT_IMAGE_NAME_LIST = [
    'cloud_mask',
    'infrared_band_133',
    'satellite_solar_radiation',
    'solar_zenith_angle',
    'visual_band_47',
    'visual_band_86'
]

# list of available subtask datasets.
AVAIL_SUBTASKNAMES_LIST = [
    'odd_time_area',
    'odd_time_point',
    'odd_space_area',
    'odd_space_point',
    'odd_spacetime_area',
    'odd_spacetime_point'
]

def load(
    local_dir: str,
    subtask_name: str,
    data_frac: Union[int, float],
    train_frac: Union[int, float],
    max_workers: int,
    seed: int = None
) -> Dict[str, List[Dict[str, Any]]]:
    """
    Load and prepare the data for a given subtask.

    Parameters
    ----------
    local_dir : str
        Local path containing the subtask data.
    subtask_name : str
        One of the recognized subtask names (see AVAIL_SUBTASKNAMES_LIST).
    data_frac : Union[int, float]
        Overall fraction of samples to keep from full dataset.
    train_frac : Union[int, float]
        Fraction of the standardized training split to actually use.
    max_workers : int
        Number of parallel workers for loading data from HDF5.
    seed : int, optional
        Random seed for reproducibility, by default None.

    Returns
    -------
    Dict[str, List[Dict[str, Any]]]
        A dictionary with keys ['train_data', 'val_data', 'test_data'].

    """
    if subtask_name not in AVAIL_SUBTASKNAMES_LIST:
        raise ValueError(f"Unknown subtask name: {subtask_name}")
        
    # 1) Load CSV data
    timestamps_dict, station_features_df, ground_radiation_df = _load_csv_data(
        local_dir, 
        subtask_name
    )

    # 2) Load HDF5 data
    satellite_images_dict = _load_hdf5_data(
        local_dir, 
        subtask_name, 
        max_workers
    )

    # 3) Create feature & label dictionaries
    features, labels = _process_features_labels(
        timestamps_dict,
        station_features_df, 
        ground_radiation_df,
        satellite_images_dict,
        subtask_name
    )

    # free memory we no longer need
    del timestamps_dict, station_features_df, ground_radiation_df
    del satellite_images_dict
    gc.collect()

    # 4) Pair features & labels into a sample-based dataset
    paired_dataset = _pair_features_labels(features, labels)
    del features, labels
    gc.collect()

    # 5) Split data according to SPLIT_RATIO and apply train_frac
    train_data, val_data, test_data = _split_data(
        paired_dataset,  
        data_frac, 
        train_frac,
        subtask_name,
        seed=seed
    )
    del paired_dataset
    gc.collect()

    # Load natural language descriptions
    task_description    = _create_taskdescription()
    subtask_description = _create_subtaskdescription(subtask_name)

    return {
        'train_data': train_data,
        'val_data': val_data,
        'test_data': test_data,
        'task_description': task_description,
        'subtask_description': subtask_description
    }


def _process_features_labels(
    timestamps_dict: dict,
    station_features_df: pd.DataFrame,
    ground_radiation_df: pd.DataFrame | None,
    satellite_images_dict: dict,
    subtask_name: str
):
    """
    Create features and labels for each station.

    Returns
    -------
    features : dict of {station_id -> dict}
    labels : dict of {station_id -> np.ndarray}

    """
    features = {}
    labels = {}

    # 'satellite_images_dict' keys like: 'station_1', 'station_2', ...
    for idx, (station_key, data_dict) in enumerate(satellite_images_dict.items()):
        station_id = idx + 1

        # gather timestamps
        timestamp_utc = timestamps_dict[station_id]['utc_time'].values.tolist()
        timestamp_local = timestamps_dict[station_id]['local_time'].values.tolist()

        # gather static info from station_features_df
        latitude = station_features_df['lats'][idx]
        longitude = station_features_df['lons'][idx]
        elevation = station_features_df['elev'][idx]

        if ground_radiation_df is not None:
            # point-level tasks
            features_satellite = np.stack(
                [
                    data_dict['infrared_band_133'],
                    data_dict['visual_band_47'],
                    data_dict['visual_band_86'],
                    data_dict['solar_zenith_angle']
                ],
                axis=-1
            )
            features_ground = ground_radiation_df[str(station_id)].to_numpy()
            label_task = ground_radiation_df[str(station_id)].to_numpy()

        else:
            # area-level tasks
            features_satellite = np.stack(
                [
                    data_dict['infrared_band_133'],
                    data_dict['visual_band_47'],
                    data_dict['visual_band_86'],
                    data_dict['solar_zenith_angle'],
                    data_dict['cloud_mask'],
                    data_dict['satellite_solar_radiation']
                ],
                axis=-1
            )
            features_ground = None
            # define label for area tasks
            label_task = (
                data_dict['satellite_solar_radiation'] *
                data_dict['cloud_mask']
            )

        features[station_id] = {
            'timestamp_utc': timestamp_utc,
            'timestamp_local': timestamp_local,
            'latitude': latitude,
            'longitude': longitude,
            'elevation': elevation,
            'features_satellite': features_satellite,
            'features_ground': features_ground
        }
        labels[station_id] = label_task

    return features, labels


def _pair_features_labels(
    features: Dict[int, Dict[str, Any]],
    labels: Dict[int, np.ndarray]
) -> Dict[int, List[Dict[str, Any]]]:
    """
    Convert each station's time series into a list of (X, y) samples 
    using a day-ahead approach (as an example).

    Returns
    -------
    paired_dataset : dict 
      station_id -> List of sample dicts

    """
    paired_dataset = {}

    for station_id, feat_dict in features.items():
        sat = feat_dict['features_satellite']  # shape: (T, 120, 120, channels)
        grd = feat_dict['features_ground']     # shape: (T,) or None
        lbl = labels[station_id]               # shape: (T,) or (T, ...)

        t_utc = feat_dict['timestamp_utc']     # length T
        t_loc = feat_dict['timestamp_local']   # length T

        T = sat.shape[0]
        station_samples = []
        # Day-ahead approach: X in [t : t+96], y in [t+96 : t+192]
        max_start = T - 2*TIME_WINDOW_STEPS

        for t in range(max_start + 1):
            x_sat = sat[t : t+TIME_WINDOW_STEPS]
            x_grd = grd[t : t+TIME_WINDOW_STEPS] if grd is not None else None
            y_lbl = lbl[t+TIME_WINDOW_STEPS : t+2*TIME_WINDOW_STEPS]

            sample = {
                'X_satellite': x_sat,
                'X_ground': x_grd,
                'y': y_lbl,
                'timestamp_utc': t_utc[t : t+TIME_WINDOW_STEPS],
                'timestamp_local': t_loc[t : t+TIME_WINDOW_STEPS],
                'latitude': feat_dict['latitude'],
                'longitude': feat_dict['longitude'],
                'elevation': feat_dict['elevation']
            }
            station_samples.append(sample)

        paired_dataset[station_id] = station_samples

    return paired_dataset


def _split_data(
    paired_dataset: Dict[int, List[Dict[str, Any]]],
    data_frac: Union[int, float],
    train_frac: Union[int, float],
    subtask_name: str,
    seed: int = None
) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]], List[Dict[str, Any]]]:
    """
    Splits data into train, val, test for the six “odd” sub-tasks:
      - odd_time_{area/point}
      - odd_space_{area/point}
      - odd_spacetime_{area/point}

    Following the definitions:
      - odd_time_* : test time stamps are NOT in training data (time-based OOD)
      - odd_space_* : test stations are NOT in training data (station-based OOD)
      - odd_spacetime_* : test stations AND test times are OOD

    The final sizes of train, val, test follow SPLIT_RATIO = (0.5, 0.1, 0.4),
    except we then apply `train_frac` (0..1) to shrink only the train set.
    Additionally, we apply `data_frac` as a fraction of the entire dataset
    to keep overall.

    Parameters
    ----------
    paired_dataset : dict
        station_id -> list of sample dicts (each dict has X, y, timestamps, etc.)
    data_frac : float or int
        fraction (0..1) of total samples to keep
    train_frac : float or int
        fraction (0..1) of how many train samples to retain
    subtask_name : str
        e.g., 'odd_time_area', 'odd_space_point', etc.
    seed : int, optional
        random seed for reproducibility, by default None

    Returns
    -------
    train_data, val_data, test_data : list of sample dicts

    """
    # ----------------------------
    # 1) Flatten station -> samples into a list for convenience
    # ----------------------------
    station_sample_pairs = []
    for st_id, samples in paired_dataset.items():
        for s in samples:
            station_sample_pairs.append((st_id, s))

    total_samples = len(station_sample_pairs)
    if total_samples == 0:
        return [], [], []

    # ----------------------------
    # 2) For the “space” subtasks, we do a station-based split.
    #    For the “time” subtasks, we do a time-based split across all stations.
    #    For “spacetime”, we do both: some stations are entirely test, and
    #    the remaining stations are split by time for train/val.
    # ----------------------------

    # Helper function: apply data_frac
    def apply_data_frac(samples_list, frac_or_count):
        """Keep only a fraction or absolute count of samples."""
        if not samples_list:
            return []
        n = len(samples_list)
        if 0 < frac_or_count < 1:
            keep = int(frac_or_count * n)
        else:
            keep = min(int(frac_or_count), n)
        return samples_list[:keep]

    # We will fill these final lists:
    train_data = []
    val_data = []
    test_data = []

    # Station-based sub-split if needed
    def station_split(st_ids, ratio=(0.5, 0.1, 0.4)):
        """
        Given a list of station IDs in ascending order,
        split them into train_stations, val_stations, test_stations
        by the ratio.

        """
        n = len(st_ids)
        tr_size = int(ratio[0] * n)
        va_size = int(ratio[1] * n)
        # test is remainder
        te_size = n - tr_size - va_size

        train_stations = st_ids[:tr_size]
        val_stations = st_ids[tr_size : tr_size + va_size]
        test_stations = st_ids[tr_size + va_size : tr_size + va_size + te_size]
        return train_stations, val_stations, test_stations

    # Time-based sub-split if needed
    def time_split(samples_list, ratio=(0.5, 0.1, 0.4)):
        """
        Sort samples by their earliest timestamp_utc,
        then pick the first ratio[0] fraction for train,
        next ratio[1] fraction for val, remainder for test.

        """
        if not samples_list:
            return [], [], []

        # sort by earliest time
        # each sample = (station_id, sample_dict)
        # We'll define an integer time or float from the FIRST UTC time in sample
        def first_utc_as_float(s):
            # s[1]['timestamp_utc'] is the sample's list of times for X-window
            # We'll parse s[1]['timestamp_utc'][0] if it exists
            times = s[1]['timestamp_utc']
            if not times:
                return 0  # fallback
            # e.g. "2021-06-05 12:00:00" -> convert or just treat as string
            # for a proper time-based approach, parse into a datetime
            # here we'll do a naive lexicographic or numeric parse
            # for demonstration:
            return pd.to_datetime(times[0]).value

        sorted_list = sorted(samples_list, key=first_utc_as_float)

        n = len(sorted_list)
        tr_size = int(ratio[0] * n)
        va_size = int(ratio[1] * n)
        te_size = n - tr_size - va_size

        tr = sorted_list[:tr_size]
        va = sorted_list[tr_size : tr_size + va_size]
        te = sorted_list[tr_size + va_size : tr_size + va_size + te_size]
        
        return tr, va, te

    # ----------------------------
    # 3) “Odd” logic
    #    We choose how to separate train/val/test based on subtask_name
    # ----------------------------
    if 'spacetime' in subtask_name:
        # 3a) Choose a subset of stations for test entirely (e.g., 40%),
        #     then within the remaining stations, do a time-based split
        #     for train/val (50% of total -> train, 10% -> val).
        #     This ensures test is OOD in both station and time stamps.
        all_station_ids = sorted(list({p[0] for p in station_sample_pairs}))
        tr_st, va_st, te_st = station_split(all_station_ids, SPLIT_RATIO)
        # Actually, we only want 40% of stations as test, but we also want
        # train:val = 50:10 among the "train/val" stations. The station_split
        # approach respects the ratio across stations. That means we have
        # purely station-based splits. If you want to do a further time-based
        # split inside the train/val stations, see below:

        # We’ll define test_data as all samples from `te_st`.
        # Among the union of tr_st and va_st, we do time-based splitting 
        # to produce train_data vs val_data. 
        test_data = [(sid, s) for (sid, s) in station_sample_pairs if sid in te_st]
        remain = [(sid, s) for (sid, s) in station_sample_pairs if sid in tr_st or sid in va_st]

        # Now time-split `remain` for train vs val
        # So effectively 60% stations remain, and within that chunk we do 50:10:0 for train:val:test
        # We do a 50:10 split => 60% total => that ratio is 50/60 = 83.3% for train, 16.7% for val
        # For simplicity, we do the ratio across `remain` as (0.8333, 0.1667, 0)
        # or we can do it exactly with the ratio. We'll do a direct fraction approach:
        remain_train, remain_val, _ = time_split(remain, ratio=(0.8333, 0.1667, 0.0))

        train_data = remain_train
        val_data = remain_val

    elif 'time' in subtask_name:
        # 3b) Purely time-based OOD. Combine all stations, then do time-based
        train_data, val_data, test_data = time_split(station_sample_pairs, SPLIT_RATIO)

    elif 'space' in subtask_name:
        # 3c) Purely station-based OOD. We pick 50% of stations for train,
        #     10% for val, 40% for test (by station ID).
        all_station_ids = sorted(list({p[0] for p in station_sample_pairs}))
        tr_st, va_st, te_st = station_split(all_station_ids, SPLIT_RATIO)

        # gather the samples
        train_data = [(sid, s) for (sid, s) in station_sample_pairs if sid in tr_st]
        val_data = [(sid, s) for (sid, s) in station_sample_pairs if sid in va_st]
        test_data = [(sid, s) for (sid, s) in station_sample_pairs if sid in te_st]

    # ----------------------------
    # 4) Convert from (station_id, sample) pairs back to just sample dicts
    # ----------------------------
    train_data = [pair[1] for pair in train_data]
    val_data = [pair[1] for pair in val_data]
    test_data = [pair[1] for pair in test_data]

    # ----------------------------
    # 5) Apply data_frac to the entire set AFTER we have established OOD splits
    #    (i.e., we keep the ratio but scale down the total).
    #    If you'd rather apply data_frac before splitting, adapt as needed.
    # ----------------------------
    # We'll just re-concatenate train, val, test, apply data_frac, then
    # re-split in the same ratio. However, if you want to preserve the OOD
    # property exactly, you might skip data_frac or apply it only to train/val.
    all_data = [('train', x) for x in train_data] + \
               [('val', x)   for x in val_data]   + \
               [('test', x)  for x in test_data]
    rng = np.random.default_rng(seed)
    rng.shuffle(all_data)

    total_kept = len(all_data)
    if 0 < data_frac < 1:
        keep_size = int(total_kept * data_frac)
    else:
        keep_size = min(int(data_frac), total_kept)

    all_data = all_data[:keep_size]

    # Now we re-group into train/val/test. 
    # The simplest approach: keep items in their original “group”
    # but we do lose some from the tail. This no longer strictly preserves 
    # the exact ratio, but it is a simpler approach that still reduces data size.
    # For a precise approach that preserves OOD splits exactly in ratio,
    # you could handle each group separately. For brevity, we do a single trim.
    new_train, new_val, new_test = [], [], []
    for grp, sample in all_data:
        if   grp == 'train': new_train.append(sample)
        elif grp == 'val':   new_val.append(sample)
        else:                new_test.append(sample)

    train_data = new_train
    val_data   = new_val
    test_data  = new_test

    # ----------------------------
    # 6) Apply train_frac to reduce only the training portion
    # ----------------------------
    original_train_count = len(train_data)
    if original_train_count > 0:
        if 0 < train_frac < 1:
            keep_train_count = int(train_frac * original_train_count)
        else:
            keep_train_count = min(int(train_frac), original_train_count)
        train_data = train_data[:keep_train_count]

    return train_data, val_data, test_data



def _load_csv_data(
    local_dir: str, 
    subtask_name: str
) -> (dict, pd.DataFrame, pd.DataFrame | None):
    """
    Load CSV files:
      - timestamps for each station
      - station_features
      - ground_radiation if it's a point-based task

    Returns
    -------
    timestamps_dict : {station_id -> DataFrame}
    station_features_df : pd.DataFrame
    ground_radiation_df : pd.DataFrame or None

    """
    path_timestamps = os.path.join(local_dir, 'availability_IDs')
    timestamps_dict = {}
    for i in range(NUMBER_STATIONS):
        filename = f'station_{i+1}_timestamps.csv'
        path_timefile = os.path.join(path_timestamps, filename)
        timestamps = pd.read_csv(path_timefile)
        timestamps_dict[i+1] = timestamps

    path_station_features_df = os.path.join(local_dir, 'station_features.csv')  
    station_features_df = pd.read_csv(path_station_features_df)
    
    if 'point' in subtask_name:
        path_ground_radiation_df = os.path.join(local_dir, 'ground_radiation.csv')  
        ground_radiation_df = pd.read_csv(path_ground_radiation_df)
    else:
        ground_radiation_df = None

    return timestamps_dict, station_features_df, ground_radiation_df


def _load_hdf5_data(
    local_dir: str, 
    subtask_name: str,
    max_workers: int
):
    """
    Load HDF5 files in parallel for each station.

    Returns
    -------
    satellite_images_dict : dict
       { 'station_{i+1}': { image_name : np.ndarray, ... } }

    """

    def load_helper(local_dir, i):
        station_name = f'station_{i+1}'
        path_station = os.path.join(local_dir, station_name)
        station_images_dict = {}
        for image_name in SAT_IMAGE_NAME_LIST:
            path_file = os.path.join(path_station, image_name + '.h5')
            with h5py.File(path_file, 'r') as hf:
                data = hf.get(image_name)[:]
            station_images_dict[image_name] = data
        return station_name, station_images_dict

    satellite_images_dict = {}
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [
            executor.submit(load_helper, local_dir, i)
            for i in range(NUMBER_STATIONS)
        ]
        for f in as_completed(futures):
            station_name, data = f.result()
            satellite_images_dict[station_name] = data

    return satellite_images_dict

def _create_taskdescription():
    """Contains natural language description of task. Placeholder."""

    task_description = """
    Given geostationary satellite imagery, physics-based simulations of solar 
    radiation, and geographic information of a specific region on Earth for the 
    past 24 hours at a snapshot in time, our goal is to predict the total 
    shortwave solar radiation reaching Earth's horizontal surface with 15 minute 
    temporal and 5 kilometer spatial resolution for 24 hours into the future.

    Improving solutions for this forecasting task will enable power system 
    operators to plan and dispatch resources more efficiently, reducing reliance 
    on backup storage, new transmission lines, and costly peak power plants, key 
    requirements for integrating high shares of fluctuating and uncertain solar 
    power into the grid.

    The fundamental physical principles shaping solutions to this task include 
    fluid mechanics, as well as diurnal and seasonal cycles.

    We define six forecasting sub-tasks, each utilizing the same geostationary 
    satellite images and augmented features as inputs but differing along two 
    key dimensions: (i) whether the prediction targets point-based or area-based 
    solar radiation, and (ii) whether unseen time stamps, geographic areas, or 
    both, are included in the test set. These distinctions result in the following 
    sub-task datasets:

    odd_time_area: Predict area-based solar radiation using satellite 
    images and physics-based simulations, with test time stamps not present in 
    training data.

    odd_space_area: Predict area-based solar radiation using 
    satellite images and physics-based simulations, with test areas not present 
    in training data.

    odd_spacetime_area: Predict area-based solar radiation using 
    satellite images and physics-based simulations, with test areas and time 
    stamps not present in training data.

    odd_time_point: Predict point-based solar radiation using satellite images 
    and ground station measurements, with test time stamps not present in 
    training data.

    odd_space_point: Predict point-based solar radiation using satellite images 
    and ground station measurements, with test areas not present in training 
    data.

    odd_spacetime_point: Predict point-based solar radiation using satellite 
    images and ground station measurements, with test areas and time stamps not 
    present in training data.
    """

    return task_description

def _create_subtaskdescription(subtask_name: str):
    """Contains natural language description of subtask. Placeholder."""

    subtask_description = f"""
    Here, we are solving instances of the {subtask_name} subtask.
    """.format(subtask_name)
    
    return subtask_description