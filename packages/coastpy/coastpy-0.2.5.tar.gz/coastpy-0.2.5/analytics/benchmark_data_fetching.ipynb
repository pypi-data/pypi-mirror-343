{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Benchmarking geospatial tabular data retrieval\n",
    "\n",
    "This experiment benchmarks the efficiency of geospatial data retrieval using eight different data dissemination strategies by retrieving coastal transects for the Basque Country, Spain, from the Global Coastal Transect System. We evaluate the efficiency gains of cloud-optimized data, geospatial sorting, and metadata filtering with Spatio-Temporal Asset Catalogs across different data models (GeoPandas, Dask GeoPandas, and DuckDB) and retrieval methods (spatial join and predicate pushdown). Geospatial sorting, which involves sorting data based on quadkey (a geohash facilitating efficient spatial indexing is examined. Metadata filtering, performed on the attributes provided in the STAC collection, allows for selective retrieval of relevant data partitions. We also compare retrieval methods, including spatial join operations, which merge datasets based on their spatial relationship, and predicate pushdown, a query optimization technique that applies filters early in the data retrieval process to enhance performance by reducing the amount of data transferred and processed. For more details see Calkoen et al, Enabling Coastal Analytics at Planetary Scale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from coastpy.utils.config import configure_instance\n",
    "\n",
    "instance_type = configure_instance()\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import dask\n",
    "\n",
    "dask.config.set({\"dataframe.query-planning\": False})\n",
    "\n",
    "import tempfile\n",
    "\n",
    "import dask_geopandas\n",
    "import duckdb\n",
    "import fsspec\n",
    "import geopandas as gpd\n",
    "import hvplot.pandas\n",
    "import pandas as pd\n",
    "import pystac_client\n",
    "import shapely\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from coastpy.io.engine import STACQueryEngine\n",
    "from coastpy.stac.utils import read_snapshot\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "sas_token = os.getenv(\"AZURE_STORAGE_SAS_TOKEN\")\n",
    "account_name = \"coclico\"\n",
    "storage_options = {\"account_name\": account_name, \"sas_token\": sas_token}\n",
    "\n",
    "# Create a dataset with the transects unsorted to do some experiments\n",
    "COMPUTE_UNSORTED_GCTS = False\n",
    "RUN_QUERY_EXPERIMENT = False\n",
    "\n",
    "GCTS_CONTAINER = \"az://transects/gcts\"\n",
    "GCTS_UNSORTED_CONTAINER = \"az://transects/gcts-2000m-unsorted.parquet\"\n",
    "\n",
    "QUERY_METRICS_FP = \"az://experiments/enabling-coastal-analytics/query-metrics.parquet\"\n",
    "QUERY_STATISTICS_FP = QUERY_METRICS_FP.replace(\"metrics.parquet\", \"statistics.csv\")\n",
    "QUERY_STATISTICS_FORMATTED_FP = QUERY_STATISTICS_FP.replace(\".csv\", \"-formatted.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Connect to the STAC catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "coclico_catalog = pystac_client.Client.open(\n",
    "    \"https://coclico.blob.core.windows.net/stac/v1/catalog.json\"\n",
    ")\n",
    "gcts_collection = coclico_catalog.get_child(\"gcts\")\n",
    "gcts_extents = read_snapshot(gcts_collection, columns=[\"geometry\", \"assets\"])\n",
    "gcts_extents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Compute unsorted transct dataset\n",
    "\n",
    "To benchmark the ordering data access create unsorted transect dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPUTE_UNSORTED_GCTS:\n",
    "    # Load all transects into memory\n",
    "    gcts_hrefs = gcts_extents.href.to_list()\n",
    "    transects_ddf = dask_geopandas.read_parquet(gcts_hrefs)\n",
    "    transects = transects_ddf.compute()\n",
    "\n",
    "    # Do a random sortation of all data\n",
    "    transects = transects.sample(frac=1)\n",
    "    transects_ddf = dask_geopandas.from_geopandas(\n",
    "        transects, npartitions=transects_ddf.npartitions\n",
    "    )\n",
    "\n",
    "    # Write the unsorted data in an equal amount of partitions to cloud storage\n",
    "    for i, ddf in enumerate(transects_ddf.partitions):\n",
    "        filename = f\"part.{i}.parquet\"\n",
    "        outpath = f\"{GCTS_UNSORTED_CONTAINER}/{filename}\"\n",
    "        df = ddf.compute()\n",
    "        with fsspec.open(outpath, mode=\"wb\", **storage_options) as f:\n",
    "            df.to_parquet(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Select a region of interest to do the query experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyleaflet import Map, basemaps\n",
    "\n",
    "m = Map(basemap=basemaps.Esri.WorldImagery, scroll_wheel_zoom=True)\n",
    "m.center = 43.406241, -2.976665\n",
    "m.zoom = 9\n",
    "m.layout.height = \"800px\"\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = shapely.geometry.box(m.west, m.south, m.east, m.north)\n",
    "roi = gpd.GeoDataFrame(geometry=[roi], crs=4326)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Retrieve data for the region of interest from the unsorted data container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Function to benchmark different query methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import uuid\n",
    "\n",
    "\n",
    "def benchmark_experiment(func, *args, n=3, **kwargs):\n",
    "    \"\"\"\n",
    "    Benchmarks the given function, logging execution time and other metrics, automatically using the function's name as the experiment key.\n",
    "\n",
    "    Args:\n",
    "        func (callable): The function to benchmark.\n",
    "        *args: Positional arguments to pass to the function.\n",
    "        n (int): Number of times to run the experiment.\n",
    "        **kwargs: Keyword arguments to pass to the function.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries containing the logged experiment results.\n",
    "    \"\"\"\n",
    "    experiment_key = func.__name__  # Automatically get the function name\n",
    "    results = []\n",
    "\n",
    "    for i in range(n):\n",
    "        start_time = time.time()\n",
    "        df = func(*args, **kwargs)\n",
    "        nrows, ncols = df.shape if df is not None else (None, None)\n",
    "        end_time = time.time()\n",
    "\n",
    "        wall_clock_time = end_time - start_time\n",
    "\n",
    "        experiment_id = f\"{experiment_key}_{uuid.uuid4().hex[:8]}\"\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"experiment\": experiment_key,\n",
    "                \"experiment_id\": experiment_id,\n",
    "                \"wall_clock_time\": wall_clock_time,\n",
    "                \"nrows\": nrows,\n",
    "                \"ncols\": ncols,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Different methods for accessing GCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpd_sjoin_from_gcts_as_gpkg():\n",
    "    # Define the remote path\n",
    "    remote_path = \"az://experiments/gcts-2000m.gpkg\"\n",
    "\n",
    "    # Initialize fsspec filesystem\n",
    "    fs = fsspec.filesystem(\"az\", **storage_options)\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".gpkg\") as tmp:\n",
    "        fs.get(remote_path, tmp.name)\n",
    "\n",
    "        transects = gpd.read_file(tmp.name)\n",
    "        transects = transects.sjoin(roi).drop(columns=[\"index_right\"])\n",
    "\n",
    "    return transects\n",
    "\n",
    "\n",
    "def dask_sjoin_from_unsorted_gcts():\n",
    "    fs = fsspec.filesystem(\"az\", **storage_options)\n",
    "    files = fs.glob(GCTS_UNSORTED_CONTAINER + \"/*.parquet\")\n",
    "    transects_ddf = dask_geopandas.read_parquet(files, filesystem=fs)\n",
    "    transects = transects_ddf.sjoin(roi).compute()\n",
    "\n",
    "    return transects\n",
    "\n",
    "\n",
    "def duckdb_sjoin_from_unsorted_gcts():\n",
    "    con = duckdb.connect(database=\":memory\", read_only=False)\n",
    "    con.execute(\"INSTALL azure;\")\n",
    "    con.execute(\"LOAD azure;\")\n",
    "    con.execute(\"INSTALL spatial;\")\n",
    "    con.execute(\"LOAD spatial;\")\n",
    "\n",
    "    try:\n",
    "        con.execute(\n",
    "            f\"\"\"\n",
    "            CREATE SECRET secret1 (\n",
    "            TYPE AZURE,\n",
    "            CONNECTION_STRING '{os.getenv('CLIENT_AZURE_STORAGE_CONNECTION_STRING')}');\n",
    "        \"\"\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"Likely the secret is already available. See exception message below:\\n\\n{e}\"\n",
    "        )\n",
    "\n",
    "    roi_wkt = roi.geometry.to_wkt().iloc[0]\n",
    "\n",
    "    con.execute(\n",
    "        \"\"\"CREATE OR REPLACE TABLE transects AS SELECT * FROM read_parquet('az://transects/gcts-2000m-unsorted.parquet/*.parquet');\"\"\"\n",
    "    )\n",
    "    df = con.execute(\n",
    "        f\"\"\"SELECT * FROM transects WHERE ST_Intersects(ST_GeomFromWKB(transects.geometry), ST_GeomFromText('{roi_wkt}'))\"\"\"\n",
    "    ).fetchdf()\n",
    "    con.close()\n",
    "    return df\n",
    "\n",
    "\n",
    "def dask_sjoin_from_sorted_gcts():\n",
    "    fs = fsspec.filesystem(\"az\", **storage_options)\n",
    "    files = fs.glob(GCTS_CONTAINER + \"/minx*.parquet\")\n",
    "    transects_ddf = dask_geopandas.read_parquet(files, filesystem=fs)\n",
    "    transects = transects_ddf.sjoin(roi).compute()\n",
    "    return transects\n",
    "\n",
    "\n",
    "def duckdb_sjoin_from_sorted_gcts():\n",
    "    con = duckdb.connect(database=\":memory\", read_only=False)\n",
    "    con.execute(\"INSTALL azure;\")\n",
    "    con.execute(\"INSTALL spatial;\")\n",
    "    con.execute(\"LOAD spatial;\")\n",
    "    con.execute(\"LOAD azure;\")\n",
    "\n",
    "    try:\n",
    "        con.execute(\n",
    "            f\"\"\"\n",
    "            CREATE SECRET secret1 (\n",
    "            TYPE AZURE,\n",
    "            CONNECTION_STRING '{os.getenv('CLIENT_AZURE_STORAGE_CONNECTION_STRING')}');\n",
    "        \"\"\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"Likely the secret is already available. See exception message below:\\n\\n{e}\"\n",
    "        )\n",
    "\n",
    "    roi_wkt = roi.geometry.to_wkt().iloc[0]\n",
    "\n",
    "    con.execute(\n",
    "        \"\"\"CREATE OR REPLACE TABLE transects AS SELECT * FROM read_parquet('az://transects/gcts-2000m.parquet/minx*.parquet');\"\"\"\n",
    "    )\n",
    "    df = con.execute(\n",
    "        f\"\"\"SELECT * FROM transects WHERE ST_Intersects(ST_GeomFromWKB(transects.geometry), ST_GeomFromText('{roi_wkt}'))\"\"\"\n",
    "    ).fetchdf()\n",
    "    con.close()\n",
    "    return df\n",
    "\n",
    "\n",
    "def dask_sjoin_with_stac_filter():\n",
    "    gcts_hrefs_roi = gpd.sjoin(gcts_extents, roi).href.to_list()\n",
    "    transects_ddf = dask_geopandas.read_parquet(\n",
    "        gcts_hrefs_roi, storage_options=storage_options\n",
    "    )\n",
    "    transects = transects_ddf.sjoin(roi).compute()\n",
    "    return transects\n",
    "\n",
    "\n",
    "def duckdb_predicate_pushdown():\n",
    "    con = duckdb.connect(database=\":memory\", read_only=False)\n",
    "    con.execute(\"INSTALL azure;\")\n",
    "    con.execute(\"LOAD azure;\")\n",
    "\n",
    "    try:\n",
    "        con.execute(\n",
    "            f\"\"\"\n",
    "            CREATE SECRET secret1 (\n",
    "            TYPE AZURE,\n",
    "            CONNECTION_STRING '{os.getenv('CLIENT_AZURE_STORAGE_CONNECTION_STRING')}');\n",
    "        \"\"\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"Likely the secret is already available. See exception message below:\\n\\n{e}\"\n",
    "        )\n",
    "\n",
    "    query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM read_parquet('az://transects/gcts-2000m.parquet/minx*.parquet')\n",
    "            WHERE\n",
    "                bbox.minx <= {m.east} AND\n",
    "                bbox.miny <= {m.north} AND\n",
    "                bbox.maxx >= {m.west} AND\n",
    "                bbox.maxy >= {m.south};\n",
    "    \"\"\"\n",
    "    df = con.execute(query).fetchdf()\n",
    "    con.close()\n",
    "    return df\n",
    "\n",
    "\n",
    "def duckdb_predicate_pushdown_with_stac_filter():\n",
    "    duckdb_engine = STACQueryEngine(\n",
    "        \"https://coclico.blob.core.windows.net/stac/v1/catalog.json\",\n",
    "        \"gcts-2000m\",\n",
    "        \"azure\",\n",
    "    )\n",
    "    transects = duckdb_engine.get_data_within_bbox(m.west, m.south, m.east, m.north)\n",
    "    return transects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Define the benchmark attributs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MethodAttributes:\n",
    "    method: str\n",
    "    data_model: str\n",
    "    data_format: str\n",
    "    cloud_optimized: bool\n",
    "    geospatially_sorted: bool\n",
    "    stac_filter: bool\n",
    "\n",
    "\n",
    "def make_method_attrs_df():\n",
    "    COLUMN_ORDER = [\n",
    "        \"experiment\",\n",
    "        \"data_format\",\n",
    "        \"data_model\",\n",
    "        \"cloud_optimized\",\n",
    "        \"method\",\n",
    "        \"geospatially_sorted\",\n",
    "        \"stac_filter\",\n",
    "    ]\n",
    "\n",
    "    attrs = {\n",
    "        \"gpd_sjoin_from_gcts_as_gpkg\": MethodAttributes(\n",
    "            method=\"Spatial join\",\n",
    "            data_model=\"GeoPandas\",\n",
    "            data_format=\"gpkg\",\n",
    "            cloud_optimized=False,\n",
    "            geospatially_sorted=False,\n",
    "            stac_filter=False,\n",
    "        ),\n",
    "        \"dask_sjoin_from_unsorted_gcts\": MethodAttributes(\n",
    "            method=\"Spatial join\",\n",
    "            data_model=\"Dask GeoPandas\",\n",
    "            data_format=\"GeoParquet\",\n",
    "            cloud_optimized=True,\n",
    "            geospatially_sorted=False,\n",
    "            stac_filter=False,\n",
    "        ),\n",
    "        \"duckdb_sjoin_from_unsorted_gcts\": MethodAttributes(\n",
    "            method=\"Spatial join\",\n",
    "            data_model=\"DuckDB\",\n",
    "            data_format=\"GeoParquet\",\n",
    "            cloud_optimized=True,\n",
    "            geospatially_sorted=False,\n",
    "            stac_filter=False,\n",
    "        ),\n",
    "        \"dask_sjoin_from_sorted_gcts\": MethodAttributes(\n",
    "            method=\"Spatial join\",\n",
    "            data_model=\"Dask GeoPandas\",\n",
    "            data_format=\"GeoParquet\",\n",
    "            cloud_optimized=True,\n",
    "            geospatially_sorted=True,\n",
    "            stac_filter=False,\n",
    "        ),\n",
    "        \"duckdb_sjoin_from_sorted_gcts\": MethodAttributes(\n",
    "            method=\"Spatial join\",\n",
    "            data_model=\"DuckDB\",\n",
    "            data_format=\"GeoParquet\",\n",
    "            cloud_optimized=True,\n",
    "            geospatially_sorted=True,\n",
    "            stac_filter=False,\n",
    "        ),\n",
    "        \"dask_sjoin_with_stac_filter\": MethodAttributes(\n",
    "            method=\"Spatial join\",\n",
    "            data_model=\"Dask GeoPandas\",\n",
    "            data_format=\"GeoParquet\",\n",
    "            cloud_optimized=True,\n",
    "            geospatially_sorted=True,\n",
    "            stac_filter=True,\n",
    "        ),\n",
    "        \"duckdb_predicate_pushdown\": MethodAttributes(\n",
    "            method=\"Predicate pushdown\",\n",
    "            data_model=\"DuckDB\",\n",
    "            data_format=\"GeoParquet\",\n",
    "            cloud_optimized=True,\n",
    "            geospatially_sorted=True,\n",
    "            stac_filter=False,\n",
    "        ),\n",
    "        \"duckdb_predicate_pushdown_with_stac_filter\": MethodAttributes(\n",
    "            method=\"Predicate pushdown\",\n",
    "            data_model=\"DuckDB\",\n",
    "            data_format=\"GeoParquet\",\n",
    "            cloud_optimized=True,\n",
    "            geospatially_sorted=True,\n",
    "            stac_filter=True,\n",
    "        ),\n",
    "    }\n",
    "    df = pd.DataFrame(\n",
    "        [{\"experiment\": key, **vars(value)} for key, value in attrs.items()]\n",
    "    )\n",
    "\n",
    "    return df[COLUMN_ORDER]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Run the benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_QUERY_EXPERIMENT:\n",
    "    N = 20\n",
    "    results = []\n",
    "    # NOTE, this time's out on MSPC, so test on local machine with more memory.\n",
    "    # results.append(benchmark_experiment(gpd_sjoin_from_gcts_as_gpkg, n=2))\n",
    "    results.append(benchmark_experiment(dask_sjoin_from_unsorted_gcts, n=N))\n",
    "    results.append(benchmark_experiment(duckdb_sjoin_from_unsorted_gcts, n=N))\n",
    "    results.append(benchmark_experiment(dask_sjoin_from_sorted_gcts, n=N))\n",
    "    results.append(benchmark_experiment(duckdb_sjoin_from_sorted_gcts, n=N))\n",
    "    results.append(benchmark_experiment(dask_sjoin_with_stac_filter, n=N))\n",
    "    results.append(benchmark_experiment(duckdb_predicate_pushdown, n=N))\n",
    "    results.append(\n",
    "        benchmark_experiment(duckdb_predicate_pushdown_with_stac_filter, n=N)\n",
    "    )\n",
    "\n",
    "    # Save metrics and stats\n",
    "    metrics = []\n",
    "    for experiment in results:\n",
    "        experiment = pd.DataFrame(experiment)\n",
    "        metrics.append(experiment)\n",
    "    metrics = pd.concat(metrics).reset_index(drop=True)\n",
    "\n",
    "    with fsspec.open(QUERY_METRICS_FP, mode=\"wb\", **storage_options) as f:\n",
    "        metrics.to_parquet(f)\n",
    "\n",
    "    stats = (\n",
    "        metrics[[\"experiment\", \"wall_clock_time\"]]\n",
    "        .groupby(\"experiment\")\n",
    "        .describe()\n",
    "        .droplevel(0, axis=1)\n",
    "        .reset_index()\n",
    "        .sort_values(by=\"mean\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    method_attrs_df = make_method_attrs_df()\n",
    "\n",
    "    missing_descriptions = [\n",
    "        e\n",
    "        for e in stats.experiment.to_list()\n",
    "        if e not in method_attrs_df.experiment.to_list()\n",
    "    ]\n",
    "    if any(missing_descriptions):\n",
    "        raise ValueError(print(f\"Missing descriptions in the method_attrs_df for  {e}\"))\n",
    "\n",
    "    stats = pd.merge(method_attrs_df, stats, on=\"experiment\", how=\"left\")\n",
    "\n",
    "    with fsspec.open(QUERY_STATISTICS_FP, mode=\"w\", **storage_options) as f:\n",
    "        stats.to_csv(f, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Load metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fsspec.open(QUERY_METRICS_FP, mode=\"rb\", **storage_options) as f:\n",
    "    metrics = pd.read_parquet(f)\n",
    "\n",
    "with fsspec.open(QUERY_STATISTICS_FP, mode=\"r\", **storage_options) as f:\n",
    "    stats = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Make table with query times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_stats_df(df):\n",
    "    df = (\n",
    "        df.sort_values(\"mean\", ascending=False)\n",
    "        .assign(mean=lambda df: df[\"mean\"].round(1))\n",
    "        .assign(std=lambda df: df[\"std\"].round(1))\n",
    "        .drop(\n",
    "            columns=[\n",
    "                \"experiment\",\n",
    "                \"data_format\",\n",
    "                \"count\",\n",
    "                \"min\",\n",
    "                \"max\",\n",
    "                \"25%\",\n",
    "                \"50%\",\n",
    "                \"75%\",\n",
    "                \"max\",\n",
    "            ]\n",
    "        )\n",
    "        .rename(\n",
    "            columns={\n",
    "                \"data_model\": \"Data model\",\n",
    "                \"method\": \"Query method\",\n",
    "                \"cloud_optimized\": \"Cloud-optimized\",\n",
    "                \"geospatially_sorted\": \"Spatial sort\",\n",
    "                \"stac_filter\": \"STAC filter\",\n",
    "                \"mean\": \"mean (s)\",\n",
    "                \"std\": \"std (s)\",\n",
    "            }\n",
    "        )\n",
    "        .replace({True: \"X\", False: \"\"})\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return df[\n",
    "        [\n",
    "            \"Data model\",\n",
    "            \"Query method\",\n",
    "            \"Cloud-optimized\",\n",
    "            \"Spatial sort\",\n",
    "            \"STAC filter\",\n",
    "            \"mean (s)\",\n",
    "            \"std (s)\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "\n",
    "formatted_stats = format_stats_df(stats)\n",
    "\n",
    "# with fsspec.open(QUERY_STATISTICS_FORMATTED_FP, mode=\"w\", **storage_options) as f:\n",
    "#     formatted_stats.to_csv(f, index=False)\n",
    "\n",
    "formatted_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Make boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "import hvplot.pandas\n",
    "from bokeh.io import export_svg\n",
    "\n",
    "hv.extension(\"bokeh\")\n",
    "\n",
    "\n",
    "with fsspec.open(QUERY_METRICS_FP, mode=\"rb\", **storage_options) as f:\n",
    "    metrics = pd.read_parquet(f)\n",
    "\n",
    "rename_experiment_mapper = {\n",
    "    \"gpd_sjoin_from_gcts_as_gpkg\": \"Expt. 1\",\n",
    "    \"dask_sjoin_from_unsorted_gcts\": \"Expt. 3\",\n",
    "    \"duckdb_sjoin_from_unsorted_gcts\": \"Expt. 5\",\n",
    "    \"dask_sjoin_from_sorted_gcts\": \"Expt. 2\",\n",
    "    \"duckdb_sjoin_from_sorted_gcts\": \"Expt. 4\",\n",
    "    \"dask_sjoin_with_stac_filter\": \"Expt. 6\",\n",
    "    \"duckdb_predicate_pushdown\": \"Expt. 7\",\n",
    "    \"duckdb_predicate_pushdown_with_stac_filter\": \"Expt. 8\",\n",
    "}\n",
    "\n",
    "metrics = metrics.assign(\n",
    "    experiment=metrics.experiment.replace(rename_experiment_mapper)\n",
    ").sort_values(\"experiment\", ascending=False)\n",
    "\n",
    "boxplot = metrics.hvplot(\n",
    "    by=\"experiment\",\n",
    "    y=\"wall_clock_time\",\n",
    "    legend=False,\n",
    "    kind=\"box\",\n",
    "    xlabel=\"\",\n",
    "    ylabel=\"Execution time (s)\",\n",
    "    loglog=True,\n",
    "    invert=True,\n",
    "    rot=0,\n",
    "    grid=True,\n",
    "    bgcolor=\"\",\n",
    "    # color=\"\"\n",
    ")\n",
    "\n",
    "\n",
    "QUERY_METRICS_BOXPLOT_FP = (\n",
    "    \"az://figures/enabling-coastal-analytics/query-metrics-boxplot.svg\"\n",
    ")\n",
    "\n",
    "renderer = hv.renderer(\"bokeh\")\n",
    "bokeh_figure = renderer.get_plot(boxplot).state\n",
    "\n",
    "bokeh_figure.output_backend = \"svg\"\n",
    "bokeh_figure.background_fill_color = None\n",
    "bokeh_figure.border_fill_color = None\n",
    "\n",
    "# outpath = (pathlib.Path(\"~\") / QUERY_METRICS_BOXPLOT_FP.strip(\"az://\")).expanduser()\n",
    "# outpath.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# with outpath.open(mode=\"w\") as fp:\n",
    "#     export_svg(bokeh_figure, filename=fp.name)\n",
    "#     with fsspec.open(\n",
    "#         QUERY_METRICS_BOXPLOT_FP, mode=\"wb\", **storage_options\n",
    "#     ) as cloud_file:\n",
    "#         with open(fp.name, mode=\"rb\") as fp_read:\n",
    "#             cloud_file.write(fp_read.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:coastal]",
   "language": "python",
   "name": "conda-env-coastal-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
